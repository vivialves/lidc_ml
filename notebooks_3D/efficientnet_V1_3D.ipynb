{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5694bfa4",
   "metadata": {},
   "source": [
    "# CNN - EfficientNet - 3D CNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc107c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import cv2\n",
    "import pydicom\n",
    "import hashlib\n",
    "import csv\n",
    "import collections\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "import SimpleITK as sitk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "\n",
    "from monai.transforms import (\n",
    "    Compose, Resize, LoadImaged, RepeatChanneld, ScaleIntensity, ResizeWithPadOrCropd, ToTensord,\n",
    "    RandGaussianNoise, RandAdjustContrast, RandGaussianSmooth, Rand3DElasticd, RandBiasField, \n",
    "    RandCropByPosNegLabeld, Resized, RandFlip, RandAffine, Compose, Resize, RandRotate, RandZoom\n",
    ")\n",
    "\n",
    "from torch.amp import autocast, GradScaler\n",
    "from monai.data import DataLoader, Dataset\n",
    "from collections import Counter\n",
    "from torchvision import models\n",
    "from torchinfo import summary\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import random_split, DataLoader, Dataset, WeightedRandomSampler\n",
    "from torchmetrics.classification import MulticlassConfusionMatrix, MulticlassPrecision, MulticlassRecall, MulticlassF1Score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report, roc_auc_score, roc_curve\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b1a510",
   "metadata": {},
   "source": [
    "# GPU Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d05b9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"PyTorch is using GPU!\")\n",
    "    print(torch.cuda.get_device_name(0)) #prints the name of the GPU.\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"PyTorch is using CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec373382",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b849e088",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = (128, 128, 128)\n",
    "BATCH_SIZE = 1\n",
    "NUM_CHANNELS = 1\n",
    "DEPTH = 64\n",
    "NUM_CLASSES = 2\n",
    "PATIENCE_COUNTER = 6\n",
    "EPOCHS = 50\n",
    "SEED = 42\n",
    "VAL_RATIO = 0.2\n",
    "TEST_RATIO = 0.2\n",
    "\n",
    "DICOM_DIR = \"/home/vivianea/projects/BrainInnov/data/LIDC_classes_dcm\"\n",
    "PATH_MODEL = '/home/vivianea/projects/BrainInnov/models/best_model_efficientnet_pytorch3D_architecture.pth'\n",
    "\n",
    "SAVE_DIR = \"/home/vivianea/projects/BrainInnov/py_files_3D/\"\n",
    "PATH_RESULTS = \"/home/vivianea/projects/BrainInnov/py_files_3D/efficientnet/results\"\n",
    "CLASS_MAP = {'cancer': 0, 'non-cancer': 1}\n",
    "INDEX_TO_CLASS = {0: 'non-cancer', 1: 'cancer'}\n",
    "\n",
    "AUG_PER_CLASS = {\"train\": 0, \"val\": 0, \"test\": 0}\n",
    "\n",
    "IMAGE_SIZE_SUMMARY = 128\n",
    "\n",
    "NUM_AUG_PER_SAMPLE = 60\n",
    "\n",
    "LOG_FILE = \"training_log.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358a6b41",
   "metadata": {},
   "source": [
    "# Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca390710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "def resize_volume(volume, target_shape):\n",
    "    resize = Resize(spatial_size=target_shape, mode=\"trilinear\")\n",
    "    resized = resize(volume)\n",
    "    if isinstance(resized, np.ndarray):\n",
    "        return resized\n",
    "    elif hasattr(resized, \"numpy\"):\n",
    "        return resized.numpy()\n",
    "    else:\n",
    "        return np.asarray(resized)\n",
    "\n",
    "def normalize_volume(volume):\n",
    "    volume = np.clip(volume, -1000, 150)\n",
    "    min_val = np.min(volume)\n",
    "    max_val = np.max(volume)\n",
    "    if max_val - min_val > 0:\n",
    "        volume = (volume - min_val) / (max_val - min_val)\n",
    "    else:\n",
    "        volume = np.zeros_like(volume)\n",
    "    return volume.astype(np.float32)\n",
    "\n",
    "\n",
    "# Helper: Load and preprocess a 3D volume from a list of DICOM paths\n",
    "def load_dicom_volume(dcm_paths, target_size=IMAGE_SIZE, min_slices=3):\n",
    "    slices = []\n",
    "    for path in dcm_paths:\n",
    "        try:\n",
    "            ds = pydicom.dcmread(path, stop_before_pixels=False)\n",
    "            if hasattr(ds, 'InstanceNumber'):\n",
    "                slices.append((ds.InstanceNumber, path))\n",
    "        except Exception:\n",
    "            print(f\"DICOM read failed: {path} | {e}\")\n",
    "            continue\n",
    "\n",
    "    if len(slices) < min_slices:\n",
    "        return None\n",
    "\n",
    "    slices.sort(key=lambda x: x[0])\n",
    "    sorted_paths = [p for _, p in slices]\n",
    "\n",
    "    volume = []\n",
    "    for path in sorted_paths:\n",
    "        try:\n",
    "            img = sitk.ReadImage(path)\n",
    "            array = sitk.GetArrayFromImage(img)[0]  # (H, W)\n",
    "            expected_shape = (512, 512) # This is the expected shape form he dicom images, because there is different shapes that don`t correspond the right folder`\n",
    "            if array.shape == expected_shape:  \n",
    "                volume.append(array)\n",
    "        except Exception as e:\n",
    "            # print(f\"Slice read failed in {path}: {e}\")\n",
    "            continue\n",
    "\n",
    "    if len(volume) < min_slices:\n",
    "        return None\n",
    "    \n",
    "    volume = np.stack(volume, axis=0)  # (D, H, W)\n",
    "    volume = np.transpose(volume, (1, 2, 0))  # (H, W, D)\n",
    "    volume = normalize_volume(volume)\n",
    "        \n",
    "    volume = np.expand_dims(volume, axis = 0)  # Add channel for Resize\n",
    "    volume = resize_volume(volume, target_size)\n",
    "    # volume = volume[0]  # Remove channel dim\n",
    "    return volume.astype(np.float32)\n",
    "\n",
    "# Helper: Build patient-to-path mapping\n",
    "def build_patient_dict(base_dir):\n",
    "    class_dict = {\"cancer\": {}, \"non-cancer\": {}}\n",
    "    for cls in [\"cancer\", \"non-cancer\"]:\n",
    "        cls_path = os.path.join(base_dir, cls)\n",
    "        for root, _, files in os.walk(cls_path):\n",
    "            for fname in files:\n",
    "                if fname.endswith(\".dcm\"):\n",
    "                    pid = fname.split(\"_\")[0]\n",
    "                    if pid not in class_dict[cls]:\n",
    "                        class_dict[cls][pid] = []\n",
    "                    class_dict[cls][pid].append(os.path.join(root, fname))\n",
    "    return class_dict\n",
    "\n",
    "def split_data(class_dict):\n",
    "    train, val, test = [], [], []\n",
    "    for label_name, pid_dict in class_dict.items():\n",
    "        label = 1 if label_name == \"cancer\" else 0\n",
    "        pids = list(pid_dict.keys())\n",
    "        train_p, test_p = train_test_split(pids, test_size=TEST_RATIO, random_state=SEED)\n",
    "        train_p, val_p = train_test_split(train_p, test_size=VAL_RATIO / (1 - TEST_RATIO), random_state=SEED)\n",
    "\n",
    "        for pid in train_p:\n",
    "            train.append((pid_dict[pid], label))\n",
    "        for pid in val_p:\n",
    "            val.append((pid_dict[pid], label))\n",
    "        for pid in test_p:\n",
    "            test.append((pid_dict[pid], label))\n",
    "\n",
    "    return train, val, test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba879d1c",
   "metadata": {},
   "source": [
    " # Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8745d84a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded: 93 train | 32 val | 32 test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying one-time augmentation: 100%|██████████| 93/93 [02:00<00:00,  1.29s/it]\n",
      "Applying one-time augmentation:  19%|█▉        | 6/32 [00:09<00:36,  1.41s/it]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "class LIDCDataset3D(Dataset):\n",
    "    def __init__(self, data_list, label_map, transform=None, seed=None):\n",
    "        self.data_list = data_list  # [(paths, label), ...]\n",
    "        self.label_map = label_map\n",
    "        self.transform = transform\n",
    "\n",
    "        self.class_to_idx = {0: \"non-cancer\", 1: \"cancer\"}\n",
    "\n",
    "        self.seed = seed\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.seed is not None:\n",
    "            # Set a deterministic seed for this item\n",
    "            seed = self.seed + idx\n",
    "            random.seed(seed)\n",
    "            np.random.seed(seed)\n",
    "            torch.manual_seed(seed)\n",
    "            \n",
    "        paths, label = self.data_list[idx]\n",
    "        vol = load_dicom_volume(paths)\n",
    "        if vol is None:\n",
    "            return self.__getitem__((idx + 1) % len(self))  # fallback\n",
    "        vol = torch.tensor(vol)\n",
    "        if self.transform:\n",
    "            vol = self.transform(vol)\n",
    "\n",
    "        label = int(label)\n",
    "        return vol, torch.tensor(label, dtype=torch.float32)\n",
    "    \n",
    "def get_transforms():\n",
    "    return Compose([\n",
    "        RandFlip(spatial_axis=0, prob=0.5),\n",
    "        RandFlip(spatial_axis=1, prob=0.5),\n",
    "        RandFlip(spatial_axis=2, prob=0.5),\n",
    "        RandRotate(range_x=0.2, range_y=0.2, range_z=0.2, prob=0.5),\n",
    "        RandAffine(\n",
    "            rotate_range=(0.1, 0.1, 0.1),\n",
    "            translate_range=(5, 5, 5),\n",
    "            scale_range=(0.1, 0.1, 0.1),\n",
    "            prob=0.3\n",
    "        ),\n",
    "        RandBiasField(prob=0.3),\n",
    "        RandAdjustContrast(prob=0.3, gamma=(0.7, 1.5)),\n",
    "        RandZoom(min_zoom=0.9, max_zoom=1.1, prob=0.2),\n",
    "        Resize(spatial_size=IMAGE_SIZE, mode=\"trilinear\")\n",
    "    ])\n",
    "\n",
    "class_dict = build_patient_dict(DICOM_DIR)\n",
    "train_data, val_data, test_data = split_data(class_dict)\n",
    "\n",
    "train_dataset = LIDCDataset3D(train_data, label_map={\"cancer\": 1, \"non-cancer\": 0}, transform=get_transforms(), seed=SEED)\n",
    "val_dataset = LIDCDataset3D(val_data, label_map={\"cancer\": 1, \"non-cancer\": 0}, transform=get_transforms(), seed=SEED)\n",
    "test_dataset = LIDCDataset3D(test_data, label_map={\"cancer\": 1, \"non-cancer\": 0}, transform=get_transforms(), seed=SEED)\n",
    "\n",
    "print(f\"✅ Loaded: {len(train_dataset)} train | {len(val_dataset)} val | {len(test_dataset)} test\")\n",
    "\n",
    "class FrozenAugmentedDataset(Dataset):\n",
    "    def __init__(self, base_dataset):\n",
    "        self.data = []\n",
    "        for i in tqdm(range(len(base_dataset)), desc=\"Applying one-time augmentation\"):\n",
    "            x, y = base_dataset[i]\n",
    "            self.data.append((x, y))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "    \n",
    "frozen_dataset_train = FrozenAugmentedDataset(train_dataset)\n",
    "frozen_dataset_val = FrozenAugmentedDataset(val_dataset)\n",
    "frozen_dataset_test = FrozenAugmentedDataset(test_dataset)\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "g = torch.Generator()\n",
    "g.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2f9102",
   "metadata": {},
   "source": [
    "## Weighted Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765ea7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_train = [s[1] for s in train_dataset.data_list]  # Adjust if your dataset is not structured this way\n",
    "class_counts = Counter(labels_train)\n",
    "max_class_count = max(class_counts.values())\n",
    "num_aug_per_sample = NUM_AUG_PER_SAMPLE\n",
    "\n",
    "target_per_class = max_class_count * num_aug_per_sample\n",
    "samples_weight = [target_per_class / class_counts[label_] for label_ in labels_train]\n",
    "num_samples = len(samples_weight) * num_aug_per_sample\n",
    "sampler_train = WeightedRandomSampler(samples_weight, num_samples=num_samples, replacement=True)\n",
    "\n",
    "train_loader = DataLoader(frozen_dataset_train, batch_size=BATCH_SIZE, num_workers=0, sampler=sampler_train, worker_init_fn=seed_worker, generator=g)\n",
    "\n",
    "labels_val = [s[1] for s in val_dataset.data_list]  # Adjust if your dataset is not structured this way\n",
    "class_counts = Counter(labels_train)\n",
    "max_class_count = max(class_counts.values())\n",
    "num_aug_per_sample = NUM_AUG_PER_SAMPLE\n",
    "\n",
    "target_per_class = max_class_count * num_aug_per_sample\n",
    "samples_weight = [target_per_class / class_counts[label_] for label_ in labels_val]\n",
    "num_samples = len(samples_weight) * num_aug_per_sample\n",
    "sampler_val = WeightedRandomSampler(samples_weight, num_samples=num_samples, replacement=True)\n",
    "\n",
    "val_loader = DataLoader(frozen_dataset_val, batch_size=BATCH_SIZE, num_workers=0, sampler=sampler_val, worker_init_fn=seed_worker, generator=g)\n",
    "\n",
    "labels_test = [s[1] for s in val_dataset.data_list]\n",
    "class_counts = Counter(labels_train)\n",
    "max_class_count = max(class_counts.values())\n",
    "num_aug_per_sample = NUM_AUG_PER_SAMPLE\n",
    "\n",
    "target_per_class = max_class_count * num_aug_per_sample\n",
    "samples_weight = [target_per_class / class_counts[label_] for label_ in labels_test]\n",
    "num_samples = len(samples_weight) * num_aug_per_sample\n",
    "sampler_test = WeightedRandomSampler(samples_weight, num_samples=num_samples, replacement=True)\n",
    "\n",
    "test_loader = DataLoader(frozen_dataset_test, batch_size=BATCH_SIZE, num_workers=0, sampler=sampler_test, worker_init_fn=seed_worker, generator=g)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2f1549",
   "metadata": {},
   "source": [
    "## Size by classes in Train Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9529df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [cls for cls in train_dataset.class_to_idx.values()]\n",
    "\n",
    "# Collect labels from your dataset\n",
    "labels = [int(label) for _, label in train_dataset.data_list]\n",
    "\n",
    "# Count occurrences\n",
    "label_counts = Counter(labels)\n",
    "\n",
    "# Define your index-to-class mapping manually if needed\n",
    "index_to_class = INDEX_TO_CLASS  # adjust if different\n",
    "\n",
    "# Print counts\n",
    "print(\"\\nTraining set counts:\")\n",
    "for idx, count in label_counts.items():\n",
    "    print(f\"Class: {index_to_class[idx]}, Count: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c079873a",
   "metadata": {},
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04fbf401",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=========================================================================================================\n",
       "Layer (type:depth-idx)                                  Output Shape              Param #\n",
       "=========================================================================================================\n",
       "EfficientNet3DClassifier                                [1, 2]                    --\n",
       "├─EfficientNetBN: 1-1                                   [1, 2]                    --\n",
       "│    └─ConstantPad3d: 2-1                               [1, 1, 65, 129, 129]      --\n",
       "│    └─Conv3d: 2-2                                      [1, 32, 32, 64, 64]       864\n",
       "│    └─BatchNorm3d: 2-3                                 [1, 32, 32, 64, 64]       64\n",
       "│    └─MemoryEfficientSwish: 2-4                        [1, 32, 32, 64, 64]       --\n",
       "│    └─Sequential: 2-5                                  [1, 320, 2, 4, 4]         --\n",
       "│    │    └─Sequential: 3-1                             [1, 16, 32, 64, 64]       2,024\n",
       "│    │    └─Sequential: 3-2                             [1, 24, 16, 32, 32]       21,034\n",
       "│    │    └─Sequential: 3-3                             [1, 40, 8, 16, 16]        85,040\n",
       "│    │    └─Sequential: 3-4                             [1, 80, 4, 8, 8]          264,530\n",
       "│    │    └─Sequential: 3-5                             [1, 112, 4, 8, 8]         725,548\n",
       "│    │    └─Sequential: 3-6                             [1, 192, 2, 4, 4]         2,439,148\n",
       "│    │    └─Sequential: 3-7                             [1, 320, 2, 4, 4]         737,968\n",
       "│    └─Identity: 2-6                                    [1, 320, 2, 4, 4]         --\n",
       "│    └─Conv3d: 2-7                                      [1, 1280, 2, 4, 4]        409,600\n",
       "│    └─BatchNorm3d: 2-8                                 [1, 1280, 2, 4, 4]        2,560\n",
       "│    └─MemoryEfficientSwish: 2-9                        [1, 1280, 2, 4, 4]        --\n",
       "│    └─AdaptiveAvgPool3d: 2-10                          [1, 1280, 1, 1, 1]        --\n",
       "│    └─Dropout: 2-11                                    [1, 1280]                 --\n",
       "│    └─Sequential: 2-12                                 [1, 2]                    --\n",
       "│    │    └─Dropout: 3-8                                [1, 1280]                 --\n",
       "│    │    └─Linear: 3-9                                 [1, 2]                    2,562\n",
       "=========================================================================================================\n",
       "Total params: 4,690,942\n",
       "Trainable params: 4,690,942\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 1.31\n",
       "=========================================================================================================\n",
       "Input size (MB): 4.19\n",
       "Forward/backward pass size (MB): 586.28\n",
       "Params size (MB): 18.76\n",
       "Estimated Total Size (MB): 609.24\n",
       "========================================================================================================="
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from monai.networks.nets import EfficientNetBN\n",
    "\n",
    "class EfficientNet3DClassifier(nn.Module):\n",
    "    def __init__(self, model_name=\"efficientnet-b0\", in_channels=1, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.model = EfficientNetBN(\n",
    "            model_name=model_name,\n",
    "            spatial_dims=3,\n",
    "            in_channels=in_channels,\n",
    "            pretrained=False  # or True if you want to fine-tune\n",
    "        )\n",
    "        # Replace the classifier\n",
    "        in_features = self.model._fc.in_features\n",
    "        self.model._fc = nn.Sequential(\n",
    "            nn.Dropout(p=0.3, inplace=True),\n",
    "            nn.Linear(in_features, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = EfficientNet3DClassifier() # Binary classification\n",
    "model = model.to(device)  # If using GPU\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "\n",
    "summary(model, input_size=(BATCH_SIZE, NUM_CHANNELS, DEPTH, IMAGE_SIZE_SUMMARY, IMAGE_SIZE_SUMMARY))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3902d483",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1946d6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_augmented_volume(volume_tensor, label, index, epoch, batch_idx, save_dir=\"augmented_samples\"):\n",
    "\n",
    "    path_augmented = os.path.join(PATH_RESULTS, save_dir)\n",
    "    os.makedirs(path_augmented, exist_ok=True)\n",
    "\n",
    "    volume_np = volume_tensor.numpy()\n",
    "    depth = volume_np.shape[1]  # Assuming [C, D, H, W]\n",
    "    center_slice = depth // 2\n",
    "\n",
    "    slice_img = volume_np[0, center_slice, :, :]  # [C, D, H, W] -> [H, W]\n",
    "\n",
    "    plt.imshow(slice_img, cmap=\"gray\")\n",
    "    plt.title(f\"Epoch {epoch+1} - Sample {index} - Label: {label.item()}\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    filepath = os.path.join(path_augmented, f\"epoch{epoch+1}_batch{batch_idx}_sample{index}_label{label.item()}.png\")\n",
    "    plt.savefig(filepath)\n",
    "    plt.close()\n",
    "\n",
    "def log_message(message):\n",
    "\n",
    "    os.makedirs(PATH_RESULTS, exist_ok=True)\n",
    "    \n",
    "    with open(os.path.join(PATH_RESULTS, LOG_FILE), \"a\") as f:\n",
    "        f.write(message + \"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, num_classes, patience, path_model):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    # Metrics\n",
    "    confusion_matrix = MulticlassConfusionMatrix(num_classes=num_classes)\n",
    "    precision = MulticlassPrecision(num_classes=num_classes, average='macro')\n",
    "    recall = MulticlassRecall(num_classes=num_classes, average='macro')\n",
    "    f1_score = MulticlassF1Score(num_classes=num_classes, average='macro')\n",
    "\n",
    "    # Early stopping parameters\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_model_path = path_model\n",
    "\n",
    "    scaler = GradScaler(\"cuda\")\n",
    "\n",
    "    # --- Gradient Accumulation Parameter ---\n",
    "    # Set your desired effective batch size here.\n",
    "    # For example, if your DataLoader's batch_size is 1 and you want an effective batch size of 4,\n",
    "    # set accumulation_steps = 4.\n",
    "    # physical_batch_size = train_loader.batch_size # Get this from your DataLoader\n",
    "    # if physical_batch_size is None: # Handle cases where batch_size might be 1 by default (no explicit arg)\n",
    "    #     physical_batch_size = 1\n",
    "    # effective_batch_size = 4 # Or 8, 16, etc.\n",
    "    # accumulation_steps = effective_batch_size // physical_batch_size\n",
    "    # if accumulation_steps == 0: accumulation_steps = 1 # Ensure at least 1 step if effective_batch_size < physical_batch_size\n",
    "\n",
    "    # A simpler way: just define the number of steps directly\n",
    "    accumulation_steps = 4 # Example: Accumulate gradients over 4 mini-batches\n",
    "    \n",
    "    starting_information = (f\"\\n\\n==== Training started at {datetime.now()} ====\\n\\n\"\n",
    "                            f\"Using Gradient Accumulation with {accumulation_steps} steps.\\n\"\n",
    "                            f\"DataLoader batch size: {train_loader.batch_size}\\n\"\n",
    "                            f\"Effective batch size: {train_loader.batch_size * accumulation_steps if train_loader.batch_size else 'N/A (check DataLoader)'}\\n\")\n",
    "    print(starting_information)\n",
    "    log_message(starting_information)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "\n",
    "        epoch_hashes = set()\n",
    "        epoch_label_counter = Counter()\n",
    "\n",
    "        confusion_matrix.reset()\n",
    "        precision.reset()\n",
    "        recall.reset()\n",
    "        f1_score.reset()\n",
    "\n",
    "        for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = torch.as_tensor(labels).to(device).long().view(-1)\n",
    "\n",
    "            epoch_label_counter.update(labels.cpu().tolist())\n",
    "\n",
    "            # Clear gradients\n",
    "            # optimizer.zero_grad()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for img in inputs.cpu():\n",
    "                    img_hash = hashlib.sha1(img.numpy().tobytes()).hexdigest()\n",
    "                    epoch_hashes.add(img_hash)\n",
    "\n",
    "            max_batches_to_save = 4  # How many batches you want to save from epoch 0\n",
    "            max_samples_per_batch = 2  # How many samples per batch to save\n",
    "\n",
    "            if epoch == 0 and batch_idx < max_batches_to_save:\n",
    "                with torch.no_grad():\n",
    "                    for i in range(min(max_samples_per_batch, inputs.size(0))):\n",
    "                        sample_idx = batch_idx * max_samples_per_batch + i\n",
    "                        plot_augmented_volume(inputs[i].cpu(), labels[i].cpu(), sample_idx, epoch, batch_idx)\n",
    "\n",
    "            with autocast(\"cuda\"):\n",
    "            # feed foward\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                # Compute loss using cross entropy\n",
    "                # Assuming your criterion is suitable for outputs and labels shapes\n",
    "                # For BCEWithLogitsLoss with 2 classes, outputs might be [B, 1] and labels [B, 1] (float)\n",
    "                # Or for CrossEntropyLoss, outputs [B, num_classes] and labels [B] (long)\n",
    "                # Your current line labels = labels.float().unsqueeze(1) suggests binary classification and outputs are also [B,1]\n",
    "                # labels_for_loss = labels.float().unsqueeze(1) if labels.dim() == 1 else labels.float() # Ensure labels match output shape\n",
    "                # loss = criterion(outputs, labels_for_loss)\n",
    "                \n",
    "\n",
    "                labels_for_loss = labels.view(-1).long() if labels.ndim > 1 else labels.long()\n",
    "                loss = criterion(outputs, labels_for_loss)\n",
    "            \n",
    "                # Compute loss using cross entropy\n",
    "                # labels = labels.float().unsqueeze(1)  # shape [B] -> [B, 1]\n",
    "                # loss = criterion(outputs, labels)\n",
    "            \n",
    "            \n",
    "            scaler.scale(loss).backward() # Scale loss and perform backward pass  # backpropagation\n",
    "            \n",
    "            # --- Gradient Accumulation: Step optimizer and update scaler only after N steps ---\n",
    "            if (batch_idx + 1) % accumulation_steps == 0:\n",
    "                scaler.step(optimizer)        # Unscale gradients and perform optimizer step\n",
    "                scaler.update()               # Update the scaler for the next iteration\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            # backpropagation\n",
    "            # loss.backward()\n",
    "\n",
    "            # Update weights\n",
    "            # optimizer.step()\n",
    "\n",
    "            # Update results\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            if num_classes > 1: # Multiclass\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "            else: # Binary\n",
    "                preds = (outputs > 0).long().squeeze(1)\n",
    "\n",
    "            total_train += labels.size(0)\n",
    "            correct_train += (preds == labels).sum().item()\n",
    "\n",
    "        if (batch_idx + 1) % accumulation_steps != 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        train_losses.append(train_loss / len(train_loader))\n",
    "        train_accuracies.append(100 * correct_train / total_train)\n",
    "\n",
    "        # Evaluation part to print metrics for each epoch\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct_val = 0\n",
    "        total_val = 0\n",
    "\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = torch.as_tensor(labels).to(device).long().view(-1)\n",
    "                with autocast(\"cuda\"):\n",
    "                    outputs = model(inputs)\n",
    "                    labels = labels.view(-1).long() if labels.ndim > 1 else labels.long()\n",
    "                    loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "\n",
    "                if num_classes > 1:\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                else:\n",
    "                    preds = (outputs > 0).long().squeeze(1)\n",
    "\n",
    "                    \n",
    "                total_val += labels.size(0)\n",
    "                correct_val += (preds == labels).sum().item()\n",
    "\n",
    "                # For metrics\n",
    "                all_preds.append(preds.cpu())\n",
    "                all_labels.append(labels.long().cpu())\n",
    "\n",
    "        val_losses.append(val_loss / len(val_loader))\n",
    "        val_accuracies.append(100 * correct_val / total_val)\n",
    "\n",
    "        \n",
    "        # all_preds = torch.cat(all_preds)\n",
    "        # all_labels = torch.cat(all_labels)\n",
    "\n",
    "        all_preds = torch.cat(all_preds).view(-1)\n",
    "        all_labels = torch.cat(all_labels).view(-1)\n",
    "\n",
    "        \n",
    "        precision_value = precision(all_preds, all_labels)\n",
    "        recall_value = recall(all_preds, all_labels)\n",
    "        f1_value = f1_score(all_preds, all_labels)\n",
    "        conf_matrix = confusion_matrix(all_preds, all_labels)\n",
    "\n",
    "        # all_preds_final = torch.as_tensor(all_preds, device=device)\n",
    "        # all_labels_final = torch.as_tensor(all_labels, device=device)\n",
    "\n",
    "        # precision_value = precision(all_preds_final, all_labels_final)\n",
    "        # recall_value = recall(all_preds_final, all_labels_final)\n",
    "        # f1_value = f1_score(all_preds_final, all_labels_final)\n",
    "        # conf_matrix = confusion_matrix(all_preds_final, all_labels_final)\n",
    "\n",
    "        \n",
    "        epoch_log = (\n",
    "            f\"Epoch {epoch + 1}/{num_epochs}\\n\"\n",
    "            f\"Train Loss: {train_losses[-1]:.4f} | Train Acc: {train_accuracies[-1]:.2f}%\\n\"\n",
    "            f\"Val Loss: {val_losses[-1]:.4f} | Val Acc: {val_accuracies[-1]:.2f}%\\n\"\n",
    "            f\"Precision: {precision_value:.4f} | Recall: {recall_value:.4f} | F1 Score: {f1_value:.4f}\\n\"\n",
    "            f\"Current AMP scale: {scaler.get_scale()}\\n\"\n",
    "            f\"Unique augmented volumes seen in epoch {epoch + 1}: {len(epoch_hashes)}\\n\"\n",
    "            f\"Label distribution in training epoch: {epoch_label_counter}\\n\"\n",
    "        )\n",
    "        print(epoch_log)\n",
    "        log_message(epoch_log)\n",
    "        \n",
    "        # --- Early Stopping Logic ---\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0 # Reset patience counter\n",
    "            # Save the best model\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'best_val_loss': best_val_loss,\n",
    "                'confusion_matrix': conf_matrix.cpu().numpy(), # Save the confusion matrix of the best model\n",
    "                'scaler_state_dict': scaler.state_dict()\n",
    "                }, best_model_path)\n",
    "            print(f\"Validation loss improved. Saving best model to {best_model_path}\\n\")\n",
    "            log_message(f\"Validation loss improved. Saving best model to {best_model_path}\\n\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"Validation loss did not improve. Patience: {patience_counter}/{patience}\\n\")\n",
    "            log_message(f\"Validation loss did not improve. Patience: {patience_counter}/{patience}\\n\")\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping triggered after {epoch+1} epochs due to no improvement in validation loss for {patience} consecutive epochs.\\n\")\n",
    "            log_message(f\"Early stopping triggered after {epoch+1} epochs.\\n\")\n",
    "            break # Exit the training loop\n",
    "\n",
    "    print(\"\\nTraining complete.\")\n",
    "    # Load the best model after training is complete (either by early stopping or max epochs)\n",
    "    print(f\"Loading best model from {best_model_path} for final metrics.\")\n",
    "    log_message(\"\\nTraining complete.\")\n",
    "    log_message(f\"Loading best model from {best_model_path} for final metrics.\")\n",
    "    checkpoint = torch.load(best_model_path, weights_only=False)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    # You might not need to load optimizer state if you're just doing inference or final evaluation\n",
    "    # --- Optional: Load optimizer state if resuming training ---\n",
    "    if 'optimizer_state_dict' in checkpoint:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict']) # Load optimizer state if resuming\n",
    "    # --- Handle scaler state dict loading ---\n",
    "    if 'scaler_state_dict' in checkpoint: # Check if the key exists\n",
    "        scaler.load_state_dict(checkpoint['scaler_state_dict']) # --- AMP: Load scaler state ---\n",
    "    else:\n",
    "        print(\"Warning: 'scaler_state_dict' not found in checkpoint. This may be an older model or AMP was not used.\")    \n",
    "    final_conf_matrix = checkpoint['confusion_matrix'] # Retrieve the confusion matrix of the best model\n",
    "\n",
    "    return train_losses, val_losses, train_accuracies, val_accuracies, conf_matrix, final_conf_matrix\n",
    "\n",
    "\n",
    "t1 = time.time()\n",
    "print(f\"Using device: {device}\")\n",
    "train_losses, val_losses, train_accuracies, val_accuracies, conf_matrix, final_conf_matrix = train_model(\n",
    "    model, train_loader, val_loader, criterion, optimizer, num_epochs=EPOCHS, num_classes=NUM_CLASSES, patience=PATIENCE_COUNTER, path_model=PATH_MODEL)\n",
    "\n",
    "elapsed = time.time() - t1\n",
    "hours, rem = divmod(elapsed, 3600)\n",
    "minutes, seconds = divmod(rem, 60)\n",
    "\n",
    "summary_ = f\"######## Training Finished in {int(hours)}h {int(minutes)}m {int(seconds)}s ###########\"\n",
    "print(summary_)\n",
    "log_message(summary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc12de2",
   "metadata": {},
   "source": [
    "# Evaluating"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46f4b87",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4b979e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for data in test_loader:\n",
    "        images, labels = data[0].to(device), data[1].to(device)\n",
    "        outputs = model(images)  # shape: [N, 1]\n",
    "        probs = torch.sigmoid(outputs)  # Convert logits to probabilities\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        labels = labels.view(-1)  # Ensure shape match with predicted\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    accuracy = f'Test Accuracy on {total} images: {(correct / total) * 100:.2f}%'\n",
    "    print(accuracy)\n",
    "    log_message(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d2b1ef",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f53c3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training(train_losses, val_losses, train_accuracies, val_accuracies):\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Plot loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_losses, label='Train Loss')\n",
    "    plt.plot(epochs, val_losses, label='Val Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, train_accuracies, label='Train Accuracy')\n",
    "    plt.plot(epochs, val_accuracies, label='Val Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.legend()\n",
    "    filepath = os.path.join(PATH_RESULTS, f\"Loss_Accuracy.png\")\n",
    "    plt.savefig(filepath)\n",
    "    plt.show()\n",
    "\n",
    "plot_training(train_losses, val_losses, train_accuracies, val_accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f1ba5e",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5be1018",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = conf_matrix.cpu().numpy()\n",
    "disp = ConfusionMatrixDisplay(cm, display_labels=classes,)\n",
    "disp.plot()\n",
    "\n",
    "plt.title(\"Confusion Matrix\")\n",
    "filepath = os.path.join(PATH_RESULTS, f\"confusion_matrix.png\")\n",
    "plt.savefig(filepath, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "y_true = []\n",
    "y_probs = []  # Collect probabilities for class 1 (cancer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f213f205",
   "metadata": {},
   "source": [
    "## ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b530dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in val_loader:  # or test_loader\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(inputs)  # raw logits or softmax\n",
    "        probs = torch.softmax(outputs, dim=1)[:, 1]\n",
    "\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "# Compute AUC\n",
    "auc = roc_auc_score(y_true, y_probs)\n",
    "\n",
    "auc_score = f\"AUC: {auc:.4f}\"\n",
    "print(auc_score)\n",
    "log_message(auc_score)\n",
    "\n",
    "# ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_true, y_probs)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label=f\"AUC = {auc:.4f}\")\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "filepath = os.path.join(PATH_RESULTS, f\"ROC_Curve.png\")\n",
    "plt.savefig(filepath)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87416d48",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08852c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = MulticlassPrecision(num_classes=NUM_CLASSES, average=None)\n",
    "recall = MulticlassRecall(num_classes=NUM_CLASSES, average=None)\n",
    "f1_score = MulticlassF1Score(num_classes=NUM_CLASSES, average=None)\n",
    "\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in val_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        all_preds.append(preds.cpu())\n",
    "        all_labels.append(labels.cpu())\n",
    "\n",
    "\n",
    "all_preds = torch.cat(all_preds)\n",
    "all_labels = torch.cat(all_labels)\n",
    "\n",
    "\n",
    "per_class_precision = precision(all_preds, all_labels)\n",
    "per_class_recall = recall(all_preds, all_labels)\n",
    "per_class_f1 = f1_score(all_preds, all_labels)\n",
    "\n",
    "\n",
    "for i, name in enumerate(classes):\n",
    "    metrics = f\"Class {i}-{name}: Precision: {per_class_precision[i]:.2f}, Recall: {per_class_recall[i]:.2f}, F1-Score: {per_class_f1[i]:.2f}\"\n",
    "    print(metrics)\n",
    "    log_message(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb1b70c",
   "metadata": {},
   "source": [
    "# GradCam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2dadba3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instantiated EfficientNet3D-Pytorch\n",
      "An unexpected error occurred: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n",
      "\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n",
      "\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n",
      "\tWeightsUnpickler error: Unsupported global: GLOBAL numpy.core.multiarray._reconstruct was not an allowed global by default. Please use `torch.serialization.add_safe_globals([numpy.core.multiarray._reconstruct])` or the `torch.serialization.safe_globals([numpy.core.multiarray._reconstruct])` context manager to allowlist this global if you trust this class/function.\n",
      "\n",
      "Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # 2. Instantiate your model class\n",
    "    # Pass the SAME in_channels and out_channels that you used during training.\n",
    "    model = EfficientNet3DClassifier()\n",
    "    print(f\"Instantiated EfficientNet3D-Pytorch\")\n",
    "\n",
    "    # 3. Load the state_dict\n",
    "    state_dict = torch.load(PATH_MODEL)\n",
    "    print(f\"Loaded state_dict from {PATH_MODEL}\")\n",
    "\n",
    "    # *** IMPORTANT CHECK: Handle 'module.' prefix if you used nn.DataParallel for training ***\n",
    "    # If your model was trained using `nn.DataParallel`, the keys in the state_dict\n",
    "    # will have a 'module.' prefix (e.g., 'module.model.features.0.weight').\n",
    "    # You need to remove this prefix when loading into a single-GPU/CPU model.\n",
    "    if list(state_dict.keys())[0].startswith('module.'):\n",
    "        print(\"Removing 'module.' prefix from state_dict keys for DataParallel compatibility...\")\n",
    "        new_state_dict = collections.OrderedDict()\n",
    "        for k, v in state_dict.items():\n",
    "            name = k[7:] # remove 'module.' prefix\n",
    "            new_state_dict[name] = v\n",
    "        model.load_state_dict(new_state_dict)\n",
    "    else:\n",
    "        print(\"No 'module.' prefix found. Loading state_dict directly.\")\n",
    "        model.load_state_dict(state_dict)\n",
    "\n",
    "    # 4. Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    print(\"Model loaded successfully and set to evaluation mode.\")\n",
    "\n",
    "    # Now 'model' is a proper torch.nn.Module instance with loaded weights.\n",
    "    # You can proceed with your Grad-CAM implementation.\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Model file not found at {PATH_MODEL}. Please double-check the path.\")\n",
    "except RuntimeError as e:\n",
    "    print(f\"Runtime Error during model loading (likely mismatch in model architecture or state_dict keys): {e}\")\n",
    "    print(\"Please ensure the `in_channels` and `out_channels` passed to DenseNet3DClassifier match what was used during training.\")\n",
    "    print(\"Also, confirm if the saved model was trained with `nn.DataParallel` (leading to 'module.' prefixes).\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ff7f9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_relu_with_non_inplace(module):\n",
    "    for name, child in module.named_children():\n",
    "        if isinstance(child, torch.nn.ReLU) and child.inplace:\n",
    "            setattr(module, name, torch.nn.ReLU(inplace=False))\n",
    "        else:\n",
    "            replace_relu_with_non_inplace(child)\n",
    "\n",
    "replace_relu_with_non_inplace(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a7c05dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.ndimage\n",
    "\n",
    "class GradCAM3D:\n",
    "    def __init__(self, model, target_layer):\n",
    "        self.model = model\n",
    "        self.target_layer = target_layer\n",
    "        self.gradients = None\n",
    "        self.activations = None\n",
    "        self._register_hooks()\n",
    "\n",
    "    def _register_hooks(self):\n",
    "        def forward_hook(module, input, output):\n",
    "            self.activations = output.detach()\n",
    "\n",
    "        def backward_hook(module, grad_in, grad_out):\n",
    "            print(\"Backward hook triggered\")\n",
    "            self.gradients = grad_out[0].detach().clone()\n",
    "\n",
    "        self.target_layer.register_forward_hook(forward_hook)\n",
    "        self.target_layer.register_full_backward_hook(backward_hook)\n",
    "\n",
    "    def generate(self, input_tensor, target_class=None):\n",
    "        input_tensor = input_tensor.unsqueeze(0)  # Add batch dim if missing\n",
    "        input_tensor = input_tensor.to(next(self.model.parameters()).device)\n",
    "        input_tensor.requires_grad = True\n",
    "\n",
    "        self.model.zero_grad()\n",
    "        output = self.model(input_tensor)\n",
    "\n",
    "        if target_class is None:\n",
    "            target_class = torch.argmax(output)\n",
    "\n",
    "        loss = output[0, target_class]\n",
    "        loss.backward()\n",
    "\n",
    "        if self.gradients is None:\n",
    "            raise RuntimeError(\"Backward hook did not capture gradients.\")\n",
    "\n",
    "        weights = torch.mean(self.gradients, dim=[2, 3, 4], keepdim=True)\n",
    "        cam = torch.sum(weights * self.activations, dim=1)\n",
    "        cam = F.relu(cam)\n",
    "        cam = cam.squeeze().cpu().detach().numpy()\n",
    "\n",
    "        return cam, target_class\n",
    "\n",
    "    def visualize(self, image_tensor, cam, predicted_class, lab):\n",
    "    \n",
    "\n",
    "        image_np = image_tensor.squeeze().cpu().numpy()\n",
    "        cam_np = cam  # Already a NumPy array\n",
    "\n",
    "        # Resize CAM to match input shape\n",
    "        cam_resized = scipy.ndimage.zoom(\n",
    "            cam_np,\n",
    "            zoom=(\n",
    "                image_np.shape[0] / cam_np.shape[0],\n",
    "                image_np.shape[1] / cam_np.shape[1],\n",
    "                image_np.shape[2] / cam_np.shape[2],\n",
    "            ),\n",
    "            order=1,\n",
    "        )\n",
    "\n",
    "        center = image_np.shape[0] // 2\n",
    "        fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "        ax[0].imshow(image_np[center], cmap='gray')\n",
    "        ax[0].set_title(\"Original Slice\")\n",
    "        ax[1].imshow(image_np[center], cmap='gray')\n",
    "        ax[1].imshow(cam_resized[center], cmap='jet', alpha=0.5)\n",
    "        ax[1].set_title(\"Grad-CAM Overlay\")\n",
    "        plt.tight_layout()\n",
    "        filepath = os.path.join(PATH_RESULTS, f\"gradcam{predicted_class}-{lab}.png\")\n",
    "        plt.savefig(filepath)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588b6bd2",
   "metadata": {},
   "source": [
    "## GRADCAM Cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a60db302",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m grad_cam = GradCAM3D(model, target_layer)\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Run on one sample\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m image, label = test_dataset[\u001b[32m0\u001b[39m]  \u001b[38;5;66;03m# image should be a tensor\u001b[39;00m\n\u001b[32m      9\u001b[39m cam, predicted_class = grad_cam.generate(image)\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Show visualization\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'test_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# Choose target layer (last conv in EfficientNet_3D Pytorch CNN)\n",
    "target_layer = model.model._conv_head\n",
    "\n",
    "# Initialize GradCAM\n",
    "grad_cam = GradCAM3D(model, target_layer)\n",
    "\n",
    "# Run on one sample\n",
    "image, label = test_dataset[0]  # image should be a tensor\n",
    "cam, predicted_class = grad_cam.generate(image)\n",
    "\n",
    "# Show visualization\n",
    "grad_cam.visualize(image, cam, predicted_class, lab='cancer')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bd183d",
   "metadata": {},
   "source": [
    "## GRADCAM Non-Cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d54bea3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m grad_cam = GradCAM3D(model, target_layer)\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Run on one sample\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m image, label = test_dataset[\u001b[32m19\u001b[39m]  \u001b[38;5;66;03m# image should be a tensor\u001b[39;00m\n\u001b[32m      9\u001b[39m cam, predicted_class = grad_cam.generate(image)\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Show visualization\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'test_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# Choose target layer (last conv in EfficientNet_3D Pytorch CNN)\n",
    "target_layer = model.model._conv_head\n",
    "\n",
    "# Initialize GradCAM\n",
    "grad_cam = GradCAM3D(model, target_layer)\n",
    "\n",
    "# Run on one sample\n",
    "image, label = test_dataset[19]  # image should be a tensor\n",
    "cam, predicted_class = grad_cam.generate(image)\n",
    "\n",
    "# Show visualization\n",
    "grad_cam.visualize(image, cam, predicted_class, lab='non-cancer')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lidc_env",
   "language": "python",
   "name": "lidc_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
