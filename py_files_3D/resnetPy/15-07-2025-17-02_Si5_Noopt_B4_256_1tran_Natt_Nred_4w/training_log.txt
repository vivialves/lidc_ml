

==== Training started at 2025-07-15 17:53:22.716763 ====

Using Gradient Accumulation with 4 steps.
DataLoader batch size: 4
Effective batch size: 16

Epoch 1/100
Train Loss: 0.6378 | Train Acc: 63.66%
Val Loss: 0.9138 | Val Acc: 50.62%
Precision: 0.5460 | Recall: 0.5267 | F1 Score: 0.4596
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 1: 92
Label distribution in training epoch: Counter({1: 237, 0: 228})

Validation loss improved. Saving best model to /home/etudiant/Projets/Viviane/LIDC/models/best_model_resnet_pytorch3D_architecture_.pth

Epoch 2/100
Train Loss: 0.4533 | Train Acc: 79.57%
Val Loss: 0.7105 | Val Acc: 63.12%
Precision: 0.6625 | Recall: 0.6220 | F1 Score: 0.6025
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 2: 93
Label distribution in training epoch: Counter({1: 248, 0: 217})

Validation loss improved. Saving best model to /home/etudiant/Projets/Viviane/LIDC/models/best_model_resnet_pytorch3D_architecture_.pth

Epoch 3/100
Train Loss: 0.2804 | Train Acc: 89.89%
Val Loss: 1.1801 | Val Acc: 63.12%
Precision: 0.6523 | Recall: 0.6530 | F1 Score: 0.6312
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 3: 89
Label distribution in training epoch: Counter({0: 244, 1: 221})

Validation loss did not improve. Patience: 1/50

Epoch 4/100
Train Loss: 0.2596 | Train Acc: 89.68%
Val Loss: 1.1850 | Val Acc: 58.12%
Precision: 0.6355 | Recall: 0.6188 | F1 Score: 0.5765
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 4: 93
Label distribution in training epoch: Counter({0: 259, 1: 206})

Validation loss did not improve. Patience: 2/50

Epoch 5/100
Train Loss: 0.1327 | Train Acc: 95.48%
Val Loss: 0.8160 | Val Acc: 60.00%
Precision: 0.6164 | Recall: 0.6041 | F1 Score: 0.5908
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 5: 92
Label distribution in training epoch: Counter({0: 242, 1: 223})

Validation loss did not improve. Patience: 3/50

Epoch 6/100
Train Loss: 0.2597 | Train Acc: 90.11%
Val Loss: 1.7461 | Val Acc: 60.62%
Precision: 0.7828 | Recall: 0.5962 | F1 Score: 0.5225
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 6: 92
Label distribution in training epoch: Counter({1: 234, 0: 231})

Validation loss did not improve. Patience: 4/50

Epoch 7/100
Train Loss: 0.1776 | Train Acc: 92.04%
Val Loss: 1.5163 | Val Acc: 73.12%
Precision: 0.8178 | Recall: 0.7471 | F1 Score: 0.7193
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 7: 93
Label distribution in training epoch: Counter({0: 236, 1: 229})

Validation loss did not improve. Patience: 5/50

Epoch 8/100
Train Loss: 0.2175 | Train Acc: 92.04%
Val Loss: 0.7721 | Val Acc: 60.00%
Precision: 0.6063 | Recall: 0.6053 | F1 Score: 0.5997
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 8: 93
Label distribution in training epoch: Counter({1: 233, 0: 232})

Validation loss did not improve. Patience: 6/50

Epoch 9/100
Train Loss: 0.1498 | Train Acc: 93.55%
Val Loss: 1.0306 | Val Acc: 70.00%
Precision: 0.7108 | Recall: 0.6910 | F1 Score: 0.6891
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 9: 90
Label distribution in training epoch: Counter({0: 243, 1: 222})

Validation loss did not improve. Patience: 7/50

Epoch 10/100
Train Loss: 0.1054 | Train Acc: 95.48%
Val Loss: 2.1545 | Val Acc: 53.12%
Precision: 0.5250 | Recall: 0.5197 | F1 Score: 0.4983
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 10: 92
Label distribution in training epoch: Counter({0: 247, 1: 218})

Validation loss did not improve. Patience: 8/50

Epoch 11/100
Train Loss: 0.1392 | Train Acc: 94.62%
Val Loss: 1.5610 | Val Acc: 55.00%
Precision: 0.5538 | Recall: 0.5522 | F1 Score: 0.5475
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 11: 92
Label distribution in training epoch: Counter({1: 234, 0: 231})

Validation loss did not improve. Patience: 9/50

Epoch 12/100
Train Loss: 0.1120 | Train Acc: 95.05%
Val Loss: 1.7653 | Val Acc: 61.88%
Precision: 0.7135 | Recall: 0.6268 | F1 Score: 0.5793
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 12: 93
Label distribution in training epoch: Counter({0: 240, 1: 225})

Validation loss did not improve. Patience: 10/50

Epoch 13/100
Train Loss: 0.0788 | Train Acc: 97.63%
Val Loss: 1.1291 | Val Acc: 51.88%
Precision: 0.5191 | Recall: 0.5187 | F1 Score: 0.5165
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 13: 93
Label distribution in training epoch: Counter({1: 237, 0: 228})

Validation loss did not improve. Patience: 11/50

Epoch 14/100
Train Loss: 0.1343 | Train Acc: 95.48%
Val Loss: 2.2466 | Val Acc: 59.38%
Precision: 0.6542 | Recall: 0.5761 | F1 Score: 0.5247
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 14: 92
Label distribution in training epoch: Counter({0: 238, 1: 227})

Validation loss did not improve. Patience: 12/50

Epoch 15/100
Train Loss: 0.0875 | Train Acc: 95.91%
Val Loss: 1.6628 | Val Acc: 60.00%
Precision: 0.6091 | Recall: 0.5896 | F1 Score: 0.5761
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 15: 92
Label distribution in training epoch: Counter({1: 256, 0: 209})

Validation loss did not improve. Patience: 13/50

Epoch 16/100
Train Loss: 0.0966 | Train Acc: 96.99%
Val Loss: 1.3269 | Val Acc: 60.62%
Precision: 0.6331 | Recall: 0.5965 | F1 Score: 0.5724
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 16: 92
Label distribution in training epoch: Counter({0: 236, 1: 229})

Validation loss did not improve. Patience: 14/50

Epoch 17/100
Train Loss: 0.0734 | Train Acc: 97.85%
Val Loss: 0.8893 | Val Acc: 53.75%
Precision: 0.5377 | Recall: 0.5363 | F1 Score: 0.5328
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 17: 92
Label distribution in training epoch: Counter({0: 251, 1: 214})

Validation loss did not improve. Patience: 15/50

Epoch 18/100
Train Loss: 0.0684 | Train Acc: 97.20%
Val Loss: 1.7764 | Val Acc: 51.25%
Precision: 0.5234 | Recall: 0.5227 | F1 Score: 0.5113
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 18: 92
Label distribution in training epoch: Counter({1: 241, 0: 224})

Validation loss did not improve. Patience: 16/50

Epoch 19/100
Train Loss: 0.0605 | Train Acc: 97.42%
Val Loss: 1.7514 | Val Acc: 55.62%
Precision: 0.6003 | Recall: 0.5562 | F1 Score: 0.5016
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 19: 93
Label distribution in training epoch: Counter({0: 233, 1: 232})

Validation loss did not improve. Patience: 17/50

Epoch 20/100
Train Loss: 0.0686 | Train Acc: 97.20%
Val Loss: 0.9985 | Val Acc: 63.12%
Precision: 0.6220 | Recall: 0.6180 | F1 Score: 0.6187
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 20: 91
Label distribution in training epoch: Counter({1: 234, 0: 231})

Validation loss did not improve. Patience: 18/50

Epoch 21/100
Train Loss: 0.0485 | Train Acc: 98.28%
Val Loss: 1.8996 | Val Acc: 45.62%
Precision: 0.4695 | Recall: 0.4697 | F1 Score: 0.4562
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 21: 92
Label distribution in training epoch: Counter({0: 241, 1: 224})

Validation loss did not improve. Patience: 19/50

Epoch 22/100
Train Loss: 0.0575 | Train Acc: 97.85%
Val Loss: 1.1476 | Val Acc: 62.50%
Precision: 0.6309 | Recall: 0.6067 | F1 Score: 0.5972
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 22: 93
Label distribution in training epoch: Counter({1: 260, 0: 205})

Validation loss did not improve. Patience: 20/50

Epoch 23/100
Train Loss: 0.0497 | Train Acc: 98.71%
Val Loss: 1.7719 | Val Acc: 57.50%
Precision: 0.5398 | Recall: 0.5354 | F1 Score: 0.5330
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 23: 92
Label distribution in training epoch: Counter({1: 241, 0: 224})

Validation loss did not improve. Patience: 21/50

Epoch 24/100
Train Loss: 0.0244 | Train Acc: 99.35%
Val Loss: 1.8970 | Val Acc: 49.38%
Precision: 0.5268 | Recall: 0.5168 | F1 Score: 0.4543
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 24: 93
Label distribution in training epoch: Counter({0: 235, 1: 230})

Validation loss did not improve. Patience: 22/50

Epoch 25/100
Train Loss: 0.0391 | Train Acc: 98.92%
Val Loss: 1.7785 | Val Acc: 59.38%
Precision: 0.5921 | Recall: 0.5938 | F1 Score: 0.5911
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 25: 93
Label distribution in training epoch: Counter({1: 237, 0: 228})

Validation loss did not improve. Patience: 23/50

Epoch 26/100
Train Loss: 0.0120 | Train Acc: 99.57%
Val Loss: 1.1961 | Val Acc: 64.38%
Precision: 0.6484 | Recall: 0.6425 | F1 Score: 0.6397
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 26: 92
Label distribution in training epoch: Counter({0: 234, 1: 231})

Validation loss did not improve. Patience: 24/50

Epoch 27/100
Train Loss: 0.0233 | Train Acc: 98.92%
Val Loss: 1.4045 | Val Acc: 62.50%
Precision: 0.6279 | Recall: 0.6250 | F1 Score: 0.6229
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 27: 91
Label distribution in training epoch: Counter({0: 245, 1: 220})

Validation loss did not improve. Patience: 25/50

Epoch 28/100
Train Loss: 0.0139 | Train Acc: 99.35%
Val Loss: 1.5022 | Val Acc: 55.00%
Precision: 0.5603 | Recall: 0.5587 | F1 Score: 0.5489
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 28: 93
Label distribution in training epoch: Counter({0: 233, 1: 232})

Validation loss did not improve. Patience: 26/50

Epoch 29/100
Train Loss: 0.0282 | Train Acc: 98.92%
Val Loss: 2.3043 | Val Acc: 61.88%
Precision: 0.6613 | Recall: 0.6081 | F1 Score: 0.5793
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 29: 93
Label distribution in training epoch: Counter({1: 248, 0: 217})

Validation loss did not improve. Patience: 27/50

Epoch 30/100
Train Loss: 0.0104 | Train Acc: 99.78%
Val Loss: 1.8986 | Val Acc: 51.88%
Precision: 0.5085 | Recall: 0.5082 | F1 Score: 0.5067
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 30: 93
Label distribution in training epoch: Counter({1: 239, 0: 226})

Validation loss did not improve. Patience: 28/50

Epoch 31/100
Train Loss: 0.0032 | Train Acc: 100.00%
Val Loss: 1.7598 | Val Acc: 52.50%
Precision: 0.5293 | Recall: 0.5278 | F1 Score: 0.5202
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 31: 91
Label distribution in training epoch: Counter({1: 239, 0: 226})

Validation loss did not improve. Patience: 29/50

Epoch 32/100
Train Loss: 0.0045 | Train Acc: 100.00%
Val Loss: 1.4223 | Val Acc: 59.38%
Precision: 0.5948 | Recall: 0.5942 | F1 Score: 0.5934
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 32: 92
Label distribution in training epoch: Counter({0: 245, 1: 220})

Validation loss did not improve. Patience: 30/50

Epoch 33/100
Train Loss: 0.0055 | Train Acc: 99.78%
Val Loss: 2.8696 | Val Acc: 44.38%
Precision: 0.4475 | Recall: 0.4468 | F1 Score: 0.4432
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 33: 92
Label distribution in training epoch: Counter({1: 234, 0: 231})

Validation loss did not improve. Patience: 31/50

Epoch 34/100
Train Loss: 0.0357 | Train Acc: 99.35%
Val Loss: 1.7222 | Val Acc: 63.12%
Precision: 0.6817 | Recall: 0.6243 | F1 Score: 0.5964
Current AMP scale: 16384.0
Unique augmented volumes seen in epoch 34: 90
Label distribution in training epoch: Counter({1: 233, 0: 232})

Validation loss did not improve. Patience: 32/50

Epoch 35/100
Train Loss: 0.0082 | Train Acc: 99.78%
Val Loss: 1.8451 | Val Acc: 56.25%
Precision: 0.5646 | Recall: 0.5642 | F1 Score: 0.5622
Current AMP scale: 16384.0
Unique augmented volumes seen in epoch 35: 92
Label distribution in training epoch: Counter({0: 243, 1: 222})

Validation loss did not improve. Patience: 33/50

Epoch 36/100
Train Loss: 0.0087 | Train Acc: 99.35%
Val Loss: 1.0332 | Val Acc: 66.25%
Precision: 0.6617 | Recall: 0.6617 | F1 Score: 0.6617
Current AMP scale: 16384.0
Unique augmented volumes seen in epoch 36: 92
Label distribution in training epoch: Counter({0: 235, 1: 230})

Validation loss did not improve. Patience: 34/50

Epoch 37/100
Train Loss: 0.0072 | Train Acc: 99.57%
Val Loss: 2.5752 | Val Acc: 51.25%
Precision: 0.5313 | Recall: 0.5125 | F1 Score: 0.4264
Current AMP scale: 16384.0
Unique augmented volumes seen in epoch 37: 92
Label distribution in training epoch: Counter({0: 237, 1: 228})

Validation loss did not improve. Patience: 35/50

Epoch 38/100
Train Loss: 0.0020 | Train Acc: 100.00%
Val Loss: 1.5018 | Val Acc: 53.75%
Precision: 0.5426 | Recall: 0.5420 | F1 Score: 0.5368
Current AMP scale: 16384.0
Unique augmented volumes seen in epoch 38: 92
Label distribution in training epoch: Counter({1: 234, 0: 231})

Validation loss did not improve. Patience: 36/50

Epoch 39/100
Train Loss: 0.0069 | Train Acc: 99.57%
Val Loss: 1.2373 | Val Acc: 68.12%
Precision: 0.6827 | Recall: 0.6817 | F1 Score: 0.6809
Current AMP scale: 16384.0
Unique augmented volumes seen in epoch 39: 91
Label distribution in training epoch: Counter({0: 239, 1: 226})

Validation loss did not improve. Patience: 37/50

Epoch 40/100
Train Loss: 0.0054 | Train Acc: 99.78%
Val Loss: 1.7597 | Val Acc: 55.00%
Precision: 0.5508 | Recall: 0.5500 | F1 Score: 0.5482
Current AMP scale: 16384.0
Unique augmented volumes seen in epoch 40: 93
Label distribution in training epoch: Counter({1: 237, 0: 228})

Validation loss did not improve. Patience: 38/50

Epoch 41/100
Train Loss: 0.0040 | Train Acc: 99.78%
Val Loss: 1.8747 | Val Acc: 60.62%
Precision: 0.5944 | Recall: 0.5768 | F1 Score: 0.5691
Current AMP scale: 16384.0
Unique augmented volumes seen in epoch 41: 91
Label distribution in training epoch: Counter({1: 238, 0: 227})

Validation loss did not improve. Patience: 39/50

Epoch 42/100
Train Loss: 0.0006 | Train Acc: 100.00%
Val Loss: 1.6301 | Val Acc: 59.38%
Precision: 0.5798 | Recall: 0.5728 | F1 Score: 0.5708
Current AMP scale: 16384.0
Unique augmented volumes seen in epoch 42: 93
Label distribution in training epoch: Counter({0: 233, 1: 232})

Validation loss did not improve. Patience: 40/50

Epoch 43/100
Train Loss: 0.0014 | Train Acc: 100.00%
Val Loss: 1.9340 | Val Acc: 43.75%
Precision: 0.4387 | Recall: 0.4461 | F1 Score: 0.4246
Current AMP scale: 16384.0
Unique augmented volumes seen in epoch 43: 92
Label distribution in training epoch: Counter({0: 261, 1: 204})

Validation loss did not improve. Patience: 41/50

Epoch 44/100
Train Loss: 0.0005 | Train Acc: 100.00%
Val Loss: 1.2481 | Val Acc: 71.88%
Precision: 0.7318 | Recall: 0.7188 | F1 Score: 0.7147
Current AMP scale: 16384.0
Unique augmented volumes seen in epoch 44: 92
Label distribution in training epoch: Counter({1: 248, 0: 217})

Validation loss did not improve. Patience: 42/50

Epoch 45/100
Train Loss: 0.0002 | Train Acc: 100.00%
Val Loss: 1.2508 | Val Acc: 56.88%
Precision: 0.5682 | Recall: 0.5680 | F1 Score: 0.5679
Current AMP scale: 16384.0
Unique augmented volumes seen in epoch 45: 91
Label distribution in training epoch: Counter({0: 238, 1: 227})

Validation loss did not improve. Patience: 43/50

Epoch 46/100
Train Loss: 0.0014 | Train Acc: 100.00%
Val Loss: 1.5293 | Val Acc: 61.25%
Precision: 0.6243 | Recall: 0.6021 | F1 Score: 0.5893
Current AMP scale: 16384.0
Unique augmented volumes seen in epoch 46: 92
Label distribution in training epoch: Counter({1: 241, 0: 224})

Validation loss did not improve. Patience: 44/50

Epoch 47/100
Train Loss: 0.0034 | Train Acc: 99.78%
Val Loss: 2.3590 | Val Acc: 56.88%
Precision: 0.6520 | Recall: 0.5855 | F1 Score: 0.5241
Current AMP scale: 16384.0
Unique augmented volumes seen in epoch 47: 92
Label distribution in training epoch: Counter({0: 248, 1: 217})

Validation loss did not improve. Patience: 45/50

Epoch 48/100
Train Loss: 0.0031 | Train Acc: 100.00%
Val Loss: 3.2838 | Val Acc: 38.75%
Precision: 0.4124 | Recall: 0.4635 | F1 Score: 0.3323
Current AMP scale: 16384.0
Unique augmented volumes seen in epoch 48: 90
Label distribution in training epoch: Counter({0: 249, 1: 216})

Validation loss did not improve. Patience: 46/50

Epoch 49/100
Train Loss: 0.0050 | Train Acc: 99.78%
Val Loss: 1.0295 | Val Acc: 72.50%
Precision: 0.7634 | Recall: 0.6880 | F1 Score: 0.6866
Current AMP scale: 16384.0
Unique augmented volumes seen in epoch 49: 92
Label distribution in training epoch: Counter({1: 257, 0: 208})

Validation loss did not improve. Patience: 47/50

Epoch 50/100
Train Loss: 0.0042 | Train Acc: 99.78%
Val Loss: 1.0287 | Val Acc: 65.00%
Precision: 0.6477 | Recall: 0.6452 | F1 Score: 0.6455
Current AMP scale: 16384.0
Unique augmented volumes seen in epoch 50: 92
Label distribution in training epoch: Counter({1: 236, 0: 229})

Validation loss did not improve. Patience: 48/50

Epoch 51/100
Train Loss: 0.0065 | Train Acc: 99.78%
Val Loss: 1.4813 | Val Acc: 67.50%
Precision: 0.7094 | Recall: 0.6842 | F1 Score: 0.6675
Current AMP scale: 16384.0
Unique augmented volumes seen in epoch 51: 92
Label distribution in training epoch: Counter({1: 237, 0: 228})

Validation loss did not improve. Patience: 49/50

Epoch 52/100
Train Loss: 0.0041 | Train Acc: 99.78%
Val Loss: 3.2478 | Val Acc: 40.00%
Precision: 0.2739 | Recall: 0.4096 | F1 Score: 0.3018
Current AMP scale: 16384.0
Unique augmented volumes seen in epoch 52: 93
Label distribution in training epoch: Counter({1: 236, 0: 229})

Validation loss did not improve. Patience: 50/50

Early stopping triggered after 52 epochs.


Training complete.
Loading best model from /home/etudiant/Projets/Viviane/LIDC/models/best_model_resnet_pytorch3D_architecture_.pth for final metrics.
######## Training Finished in 2h 57m 33s ###########
Test Accuracy on 160 images: 53.75%
AUC: 0.7271
Class 0-non-cancer: Precision: 0.59, Recall: 0.85, F1-Score: 0.70
Class 1-cancer: Precision: 0.73, Recall: 0.41, F1-Score: 0.52
