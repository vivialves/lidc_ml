

==== Training started at 2025-07-20 12:45:08.157250 ====

Using Gradient Accumulation with 4 steps.
DataLoader batch size: 2
Effective batch size: 8

Epoch 1/100
Train Loss: 0.6934 | Train Acc: 56.99%
Val Loss: 0.6901 | Val Acc: 56.25%
Precision: 0.5500 | Recall: 0.5476 | F1 Score: 0.5466
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 1: 54
Label distribution in training epoch: Counter({1: 48, 0: 45})

Validation loss improved. Saving best model to /home/etudiant/Projets/Viviane/LIDC-ML/models/best_model_resnet_pytorch3D_architecture_v0.pth

Epoch 2/100
Train Loss: 0.7166 | Train Acc: 51.61%
Val Loss: 0.8148 | Val Acc: 53.12%
Precision: 0.5227 | Recall: 0.5196 | F1 Score: 0.5077
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 2: 62
Label distribution in training epoch: Counter({1: 51, 0: 42})

Validation loss did not improve. Patience: 1/80

Epoch 3/100
Train Loss: 0.6860 | Train Acc: 59.14%
Val Loss: 0.9172 | Val Acc: 46.88%
Precision: 0.5222 | Recall: 0.5119 | F1 Score: 0.4231
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 3: 60
Label distribution in training epoch: Counter({0: 47, 1: 46})

Validation loss did not improve. Patience: 2/80

Epoch 4/100
Train Loss: 0.6859 | Train Acc: 58.06%
Val Loss: 1.2228 | Val Acc: 43.75%
Precision: 0.4365 | Recall: 0.4375 | F1 Score: 0.4353
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 4: 67
Label distribution in training epoch: Counter({0: 49, 1: 44})

Validation loss did not improve. Patience: 3/80

Epoch 5/100
Train Loss: 0.6154 | Train Acc: 67.74%
Val Loss: 0.9266 | Val Acc: 59.38%
Precision: 0.7593 | Recall: 0.6389 | F1 Score: 0.5589
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 5: 54
Label distribution in training epoch: Counter({0: 54, 1: 39})

Validation loss did not improve. Patience: 4/80

Epoch 6/100
Train Loss: 0.6828 | Train Acc: 59.14%
Val Loss: 0.6962 | Val Acc: 71.88%
Precision: 0.7188 | Recall: 0.7196 | F1 Score: 0.7185
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 6: 60
Label distribution in training epoch: Counter({1: 59, 0: 34})

Validation loss did not improve. Patience: 5/80

Epoch 7/100
Train Loss: 0.7034 | Train Acc: 63.44%
Val Loss: 1.0991 | Val Acc: 34.38%
Precision: 0.3542 | Recall: 0.3866 | F1 Score: 0.3273
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 7: 59
Label distribution in training epoch: Counter({1: 50, 0: 43})

Validation loss did not improve. Patience: 6/80

Epoch 8/100
Train Loss: 0.5962 | Train Acc: 66.67%
Val Loss: 1.0652 | Val Acc: 56.25%
Precision: 0.5686 | Recall: 0.5709 | F1 Score: 0.5608
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 8: 60
Label distribution in training epoch: Counter({0: 51, 1: 42})

Validation loss did not improve. Patience: 7/80

Epoch 9/100
Train Loss: 0.6341 | Train Acc: 62.37%
Val Loss: 0.8691 | Val Acc: 56.25%
Precision: 0.5833 | Recall: 0.5625 | F1 Score: 0.5333
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 9: 62
Label distribution in training epoch: Counter({1: 54, 0: 39})

Validation loss did not improve. Patience: 8/80

Epoch 10/100
Train Loss: 0.6484 | Train Acc: 62.37%
Val Loss: 1.2267 | Val Acc: 46.88%
Precision: 0.5893 | Recall: 0.5405 | F1 Score: 0.4231
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 10: 58
Label distribution in training epoch: Counter({0: 51, 1: 42})

Validation loss did not improve. Patience: 9/80

Epoch 11/100
Train Loss: 0.6094 | Train Acc: 68.82%
Val Loss: 0.9854 | Val Acc: 50.00%
Precision: 0.4939 | Recall: 0.4941 | F1 Score: 0.4921
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 11: 57
Label distribution in training epoch: Counter({0: 50, 1: 43})

Validation loss did not improve. Patience: 10/80

Epoch 12/100
Train Loss: 0.5387 | Train Acc: 72.04%
Val Loss: 1.3610 | Val Acc: 65.62%
Precision: 0.6562 | Recall: 0.6569 | F1 Score: 0.6559
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 12: 55
Label distribution in training epoch: Counter({0: 53, 1: 40})

Validation loss did not improve. Patience: 11/80

Epoch 13/100
Train Loss: 0.6001 | Train Acc: 73.12%
Val Loss: 1.0988 | Val Acc: 62.50%
Precision: 0.6277 | Recall: 0.6157 | F1 Score: 0.6113
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 13: 58
Label distribution in training epoch: Counter({0: 51, 1: 42})

Validation loss did not improve. Patience: 12/80

Epoch 14/100
Train Loss: 0.5577 | Train Acc: 69.89%
Val Loss: 1.0451 | Val Acc: 59.38%
Precision: 0.4829 | Recall: 0.4864 | F1 Score: 0.4793
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 14: 59
Label distribution in training epoch: Counter({1: 51, 0: 42})

Validation loss did not improve. Patience: 13/80

Epoch 15/100
Train Loss: 0.5787 | Train Acc: 72.04%
Val Loss: 1.3570 | Val Acc: 34.38%
Precision: 0.3485 | Recall: 0.3611 | F1 Score: 0.3379
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 15: 60
Label distribution in training epoch: Counter({1: 53, 0: 40})

Validation loss did not improve. Patience: 14/80

Epoch 16/100
Train Loss: 0.5482 | Train Acc: 70.97%
Val Loss: 1.2280 | Val Acc: 56.25%
Precision: 0.5143 | Recall: 0.5101 | F1 Score: 0.4909
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 16: 50
Label distribution in training epoch: Counter({0: 61, 1: 32})

Validation loss did not improve. Patience: 15/80

Epoch 17/100
Train Loss: 0.7062 | Train Acc: 67.74%
Val Loss: 1.4374 | Val Acc: 43.75%
Precision: 0.4416 | Recall: 0.4471 | F1 Score: 0.4286
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 17: 58
Label distribution in training epoch: Counter({1: 48, 0: 45})

Validation loss did not improve. Patience: 16/80

Epoch 18/100
Train Loss: 0.5428 | Train Acc: 74.19%
Val Loss: 1.8609 | Val Acc: 34.38%
Precision: 0.3250 | Recall: 0.3353 | F1 Score: 0.3273
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 18: 58
Label distribution in training epoch: Counter({0: 54, 1: 39})

Validation loss did not improve. Patience: 17/80

Epoch 19/100
Train Loss: 0.5117 | Train Acc: 78.49%
Val Loss: 3.0216 | Val Acc: 50.00%
Precision: 0.5771 | Recall: 0.5547 | F1 Score: 0.4818
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 19: 54
Label distribution in training epoch: Counter({0: 49, 1: 44})

Validation loss did not improve. Patience: 18/80

Epoch 20/100
Train Loss: 0.3744 | Train Acc: 90.32%
Val Loss: 2.1958 | Val Acc: 50.00%
Precision: 0.2500 | Recall: 0.5000 | F1 Score: 0.3333
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 20: 54
Label distribution in training epoch: Counter({0: 56, 1: 37})

Validation loss did not improve. Patience: 19/80

Epoch 21/100
Train Loss: 0.4997 | Train Acc: 76.34%
Val Loss: 2.0062 | Val Acc: 50.00%
Precision: 0.5167 | Recall: 0.5159 | F1 Score: 0.4980
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 21: 62
Label distribution in training epoch: Counter({1: 53, 0: 40})

Validation loss did not improve. Patience: 20/80

Epoch 22/100
Train Loss: 0.5128 | Train Acc: 77.42%
Val Loss: 1.9828 | Val Acc: 56.25%
Precision: 0.2812 | Recall: 0.5000 | F1 Score: 0.3600
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 22: 53
Label distribution in training epoch: Counter({0: 51, 1: 42})

Validation loss did not improve. Patience: 21/80

Epoch 23/100
Train Loss: 0.4666 | Train Acc: 74.19%
Val Loss: 1.4068 | Val Acc: 50.00%
Precision: 0.5257 | Recall: 0.5176 | F1 Score: 0.4667
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 23: 59
Label distribution in training epoch: Counter({0: 52, 1: 41})

Validation loss did not improve. Patience: 22/80

Epoch 24/100
Train Loss: 0.4340 | Train Acc: 81.72%
Val Loss: 1.1939 | Val Acc: 59.38%
Precision: 0.6042 | Recall: 0.5784 | F1 Score: 0.5589
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 24: 61
Label distribution in training epoch: Counter({1: 47, 0: 46})

Validation loss did not improve. Patience: 23/80

Epoch 25/100
Train Loss: 0.4059 | Train Acc: 78.49%
Val Loss: 2.1369 | Val Acc: 50.00%
Precision: 0.5407 | Recall: 0.5216 | F1 Score: 0.4459
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 25: 56
Label distribution in training epoch: Counter({0: 52, 1: 41})

Validation loss did not improve. Patience: 24/80

Epoch 26/100
Train Loss: 0.6113 | Train Acc: 68.82%
Val Loss: 4.7092 | Val Acc: 28.12%
Precision: 0.3225 | Recall: 0.3136 | F1 Score: 0.2805
Current AMP scale: 16384.0
Unique augmented volumes seen in epoch 26: 60
Label distribution in training epoch: Counter({1: 52, 0: 41})

Validation loss did not improve. Patience: 25/80

Epoch 27/100
Train Loss: 0.4001 | Train Acc: 82.80%
Val Loss: 2.6515 | Val Acc: 53.12%
Precision: 0.7581 | Recall: 0.5312 | F1 Score: 0.3992
Current AMP scale: 16384.0
Unique augmented volumes seen in epoch 27: 56
Label distribution in training epoch: Counter({1: 47, 0: 46})

Validation loss did not improve. Patience: 26/80

Epoch 28/100
Train Loss: 0.4587 | Train Acc: 77.42%
Val Loss: 2.1593 | Val Acc: 46.88%
Precision: 0.3958 | Recall: 0.4190 | F1 Score: 0.3976
Current AMP scale: 16384.0
Unique augmented volumes seen in epoch 28: 61
Label distribution in training epoch: Counter({1: 49, 0: 44})

Validation loss did not improve. Patience: 27/80

Epoch 29/100
Train Loss: 0.5736 | Train Acc: 73.12%
Val Loss: 2.4263 | Val Acc: 37.50%
Precision: 0.3563 | Recall: 0.4494 | F1 Score: 0.3074
Current AMP scale: 16384.0
Unique augmented volumes seen in epoch 29: 57
Label distribution in training epoch: Counter({0: 54, 1: 39})

Validation loss did not improve. Patience: 28/80

Epoch 30/100
Train Loss: 0.4273 | Train Acc: 77.42%
Val Loss: 2.2786 | Val Acc: 56.25%
Precision: 0.6167 | Recall: 0.6273 | F1 Score: 0.5608
Current AMP scale: 16384.0
Unique augmented volumes seen in epoch 30: 56
Label distribution in training epoch: Counter({1: 52, 0: 41})

Validation loss did not improve. Patience: 29/80

Epoch 31/100
Train Loss: 0.3816 | Train Acc: 82.80%
Val Loss: 1.3346 | Val Acc: 50.00%
Precision: 0.5108 | Recall: 0.5098 | F1 Score: 0.4921
Current AMP scale: 16384.0
Unique augmented volumes seen in epoch 31: 55
Label distribution in training epoch: Counter({0: 51, 1: 42})

Validation loss did not improve. Patience: 30/80

Epoch 32/100
Train Loss: 0.3314 | Train Acc: 86.02%
Val Loss: 0.9468 | Val Acc: 62.50%
Precision: 0.5952 | Recall: 0.6250 | F1 Score: 0.5844
Current AMP scale: 16384.0
Unique augmented volumes seen in epoch 32: 60
Label distribution in training epoch: Counter({1: 50, 0: 43})

Validation loss did not improve. Patience: 31/80

Epoch 33/100
Train Loss: 0.2855 | Train Acc: 87.10%
Val Loss: 3.3118 | Val Acc: 40.62%
Precision: 0.4087 | Recall: 0.4098 | F1 Score: 0.4057
Current AMP scale: 16384.0
Unique augmented volumes seen in epoch 33: 62
Label distribution in training epoch: Counter({1: 52, 0: 41})

Validation loss did not improve. Patience: 32/80

Epoch 34/100
Train Loss: 0.2683 | Train Acc: 86.02%
Val Loss: 3.2812 | Val Acc: 65.62%
Precision: 0.6594 | Recall: 0.6310 | F1 Score: 0.6267
Current AMP scale: 16384.0
Unique augmented volumes seen in epoch 34: 58
Label distribution in training epoch: Counter({1: 47, 0: 46})

Validation loss did not improve. Patience: 33/80

Epoch 35/100
Train Loss: 0.4920 | Train Acc: 82.80%
Val Loss: 7.0421 | Val Acc: 46.88%
Precision: 0.4375 | Recall: 0.4529 | F1 Score: 0.4231
Current AMP scale: 16384.0
Unique augmented volumes seen in epoch 35: 58
Label distribution in training epoch: Counter({1: 49, 0: 44})

Validation loss did not improve. Patience: 34/80

Epoch 36/100
Train Loss: 0.3466 | Train Acc: 80.65%
Val Loss: 4.2499 | Val Acc: 46.88%
Precision: 0.4167 | Recall: 0.4490 | F1 Score: 0.3976
Current AMP scale: 16384.0
Unique augmented volumes seen in epoch 36: 61
Label distribution in training epoch: Counter({1: 48, 0: 45})

Validation loss did not improve. Patience: 35/80

Epoch 37/100
Train Loss: 0.2769 | Train Acc: 89.25%
Val Loss: 2.8672 | Val Acc: 46.88%
Precision: 0.2344 | Recall: 0.5000 | F1 Score: 0.3191
Current AMP scale: 16384.0
Unique augmented volumes seen in epoch 37: 60
Label distribution in training epoch: Counter({1: 51, 0: 42})

Validation loss did not improve. Patience: 36/80

Epoch 38/100
Train Loss: 0.3353 | Train Acc: 86.02%
Val Loss: 2.5074 | Val Acc: 50.00%
Precision: 0.5325 | Recall: 0.5304 | F1 Score: 0.4980
Current AMP scale: 16384.0
Unique augmented volumes seen in epoch 38: 58
Label distribution in training epoch: Counter({0: 57, 1: 36})

Validation loss did not improve. Patience: 37/80

Epoch 39/100
Train Loss: 0.1577 | Train Acc: 93.55%
Val Loss: 1.5640 | Val Acc: 31.25%
Precision: 0.2818 | Recall: 0.3125 | F1 Score: 0.2874
Current AMP scale: 16384.0
Unique augmented volumes seen in epoch 39: 60
Label distribution in training epoch: Counter({1: 52, 0: 41})

Validation loss did not improve. Patience: 38/80

Epoch 40/100
Train Loss: 0.1753 | Train Acc: 94.62%
Val Loss: 1.1861 | Val Acc: 62.50%
Precision: 0.6250 | Recall: 0.6270 | F1 Score: 0.6235
Current AMP scale: 16384.0
Unique augmented volumes seen in epoch 40: 52
Label distribution in training epoch: Counter({1: 54, 0: 39})

Validation loss did not improve. Patience: 39/80

Epoch 41/100
Train Loss: 0.2696 | Train Acc: 88.17%
Val Loss: 2.8803 | Val Acc: 71.88%
Precision: 0.8393 | Recall: 0.6538 | F1 Score: 0.6395
Current AMP scale: 16384.0
Unique augmented volumes seen in epoch 41: 53
Label distribution in training epoch: Counter({0: 54, 1: 39})

Validation loss did not improve. Patience: 40/80

Epoch 42/100
Train Loss: 0.3550 | Train Acc: 83.87%
Val Loss: 1.1411 | Val Acc: 62.50%
Precision: 0.6250 | Recall: 0.5952 | F1 Score: 0.5844
Current AMP scale: 16384.0
Unique augmented volumes seen in epoch 42: 58
Label distribution in training epoch: Counter({0: 49, 1: 44})

Validation loss did not improve. Patience: 41/80

Epoch 43/100
Train Loss: 0.1764 | Train Acc: 90.32%
Val Loss: 1.2979 | Val Acc: 50.00%
Precision: 0.4841 | Recall: 0.4833 | F1 Score: 0.4818
Current AMP scale: 16384.0
Unique augmented volumes seen in epoch 43: 61
Label distribution in training epoch: Counter({0: 50, 1: 43})

Validation loss did not improve. Patience: 42/80

Epoch 44/100
Train Loss: 0.0901 | Train Acc: 95.70%
Val Loss: 2.7836 | Val Acc: 40.62%
Precision: 0.4517 | Recall: 0.4583 | F1 Score: 0.4010
Current AMP scale: 16384.0
Unique augmented volumes seen in epoch 44: 52
Label distribution in training epoch: Counter({1: 52, 0: 41})

Validation loss did not improve. Patience: 43/80

Epoch 45/100
Train Loss: 0.0675 | Train Acc: 97.85%
Val Loss: 1.3282 | Val Acc: 56.25%
Precision: 0.5417 | Recall: 0.5317 | F1 Score: 0.5152
Current AMP scale: 16384.0
Unique augmented volumes seen in epoch 45: 58
Label distribution in training epoch: Counter({0: 52, 1: 41})

Validation loss did not improve. Patience: 44/80

Epoch 46/100
Train Loss: 0.0620 | Train Acc: 96.77%
Val Loss: 2.2283 | Val Acc: 53.12%
Precision: 0.5392 | Recall: 0.5417 | F1 Score: 0.5271
Current AMP scale: 16384.0
Unique augmented volumes seen in epoch 46: 56
Label distribution in training epoch: Counter({0: 49, 1: 44})

Validation loss did not improve. Patience: 45/80

Epoch 47/100
Train Loss: 0.0532 | Train Acc: 98.92%
Val Loss: 3.1130 | Val Acc: 56.25%
Precision: 0.6250 | Recall: 0.5952 | F1 Score: 0.5466
Current AMP scale: 16384.0
Unique augmented volumes seen in epoch 47: 55
Label distribution in training epoch: Counter({0: 51, 1: 42})

Validation loss did not improve. Patience: 46/80

Epoch 48/100
Train Loss: 0.0305 | Train Acc: 100.00%
Val Loss: 3.3819 | Val Acc: 50.00%
Precision: 0.5061 | Recall: 0.5059 | F1 Score: 0.4980
Current AMP scale: 16384.0
Unique augmented volumes seen in epoch 48: 61
Label distribution in training epoch: Counter({0: 48, 1: 45})

Validation loss did not improve. Patience: 47/80

Epoch 49/100
Train Loss: 0.0573 | Train Acc: 96.77%
Val Loss: 2.4820 | Val Acc: 56.25%
Precision: 0.5529 | Recall: 0.5584 | F1 Score: 0.5466
Current AMP scale: 16384.0
Unique augmented volumes seen in epoch 49: 61
Label distribution in training epoch: Counter({1: 47, 0: 46})

Validation loss did not improve. Patience: 48/80

Epoch 50/100
Train Loss: 0.1419 | Train Acc: 96.77%
Val Loss: 3.3002 | Val Acc: 37.50%
Precision: 0.3723 | Recall: 0.3843 | F1 Score: 0.3651
Current AMP scale: 16384.0
Unique augmented volumes seen in epoch 50: 64
Label distribution in training epoch: Counter({1: 47, 0: 46})

Validation loss did not improve. Patience: 49/80

Epoch 51/100
Train Loss: 0.0309 | Train Acc: 98.92%
Val Loss: 3.7143 | Val Acc: 53.12%
Precision: 0.4107 | Recall: 0.4595 | F1 Score: 0.3992
Current AMP scale: 16384.0
Unique augmented volumes seen in epoch 51: 60
Label distribution in training epoch: Counter({0: 51, 1: 42})

Validation loss did not improve. Patience: 50/80

Epoch 52/100
Train Loss: 0.0541 | Train Acc: 97.85%
Val Loss: 2.9635 | Val Acc: 53.12%
Precision: 0.6474 | Recall: 0.5931 | F1 Score: 0.5077
Current AMP scale: 16384.0
Unique augmented volumes seen in epoch 52: 62
Label distribution in training epoch: Counter({0: 50, 1: 43})

Validation loss did not improve. Patience: 51/80

Epoch 53/100
Train Loss: 0.0573 | Train Acc: 97.85%
Val Loss: 1.9528 | Val Acc: 62.50%
Precision: 0.6190 | Recall: 0.6190 | F1 Score: 0.6190
Current AMP scale: 16384.0
Unique augmented volumes seen in epoch 53: 67
Label distribution in training epoch: Counter({1: 52, 0: 41})

Validation loss did not improve. Patience: 52/80

Epoch 54/100
Train Loss: 0.0279 | Train Acc: 97.85%
Val Loss: 2.9108 | Val Acc: 53.12%
Precision: 0.7414 | Recall: 0.5833 | F1 Score: 0.4684
Current AMP scale: 16384.0
Unique augmented volumes seen in epoch 54: 57
Label distribution in training epoch: Counter({0: 48, 1: 45})

Validation loss did not improve. Patience: 53/80

Epoch 55/100
Train Loss: 0.2992 | Train Acc: 90.32%
Val Loss: 2.2391 | Val Acc: 53.12%
Precision: 0.5048 | Recall: 0.5040 | F1 Score: 0.4910
Current AMP scale: 16384.0
Unique augmented volumes seen in epoch 55: 61
Label distribution in training epoch: Counter({1: 47, 0: 46})

Validation loss did not improve. Patience: 54/80

Epoch 56/100
Train Loss: 0.0745 | Train Acc: 96.77%
Val Loss: 3.7357 | Val Acc: 53.12%
Precision: 0.5278 | Recall: 0.5275 | F1 Score: 0.5271
Current AMP scale: 16384.0
Unique augmented volumes seen in epoch 56: 59
Label distribution in training epoch: Counter({1: 56, 0: 37})

Validation loss did not improve. Patience: 55/80

Epoch 57/100
Train Loss: 0.0472 | Train Acc: 97.85%
Val Loss: 3.1921 | Val Acc: 34.38%
Precision: 0.3701 | Recall: 0.3750 | F1 Score: 0.3431
Current AMP scale: 16384.0
Unique augmented volumes seen in epoch 57: 56
Label distribution in training epoch: Counter({0: 52, 1: 41})

Validation loss did not improve. Patience: 56/80

Epoch 58/100
Train Loss: 0.0630 | Train Acc: 98.92%
Val Loss: 1.9595 | Val Acc: 62.50%
Precision: 0.6314 | Recall: 0.6356 | F1 Score: 0.6235
Current AMP scale: 16384.0
Unique augmented volumes seen in epoch 58: 57
Label distribution in training epoch: Counter({0: 53, 1: 40})

Validation loss did not improve. Patience: 57/80

Epoch 59/100
Train Loss: 0.0322 | Train Acc: 98.92%
Val Loss: 6.6165 | Val Acc: 46.88%
Precision: 0.2344 | Recall: 0.5000 | F1 Score: 0.3191
Current AMP scale: 16384.0
Unique augmented volumes seen in epoch 59: 66
Label distribution in training epoch: Counter({0: 47, 1: 46})

Validation loss did not improve. Patience: 58/80

Epoch 60/100
Train Loss: 0.1316 | Train Acc: 96.77%
Val Loss: 3.8371 | Val Acc: 43.75%
Precision: 0.4534 | Recall: 0.4534 | F1 Score: 0.4375
Current AMP scale: 16384.0
Unique augmented volumes seen in epoch 60: 53
Label distribution in training epoch: Counter({1: 48, 0: 45})

Validation loss did not improve. Patience: 59/80

Epoch 61/100
Train Loss: 0.0030 | Train Acc: 100.00%
Val Loss: 1.6339 | Val Acc: 68.75%
Precision: 0.6824 | Recall: 0.6883 | F1 Score: 0.6825
Current AMP scale: 16384.0
Unique augmented volumes seen in epoch 61: 52
Label distribution in training epoch: Counter({0: 54, 1: 39})

Validation loss did not improve. Patience: 60/80

Epoch 62/100
Train Loss: 0.0123 | Train Acc: 98.92%
Val Loss: 2.4620 | Val Acc: 50.00%
Precision: 0.5061 | Recall: 0.5059 | F1 Score: 0.4980
Current AMP scale: 16384.0
Unique augmented volumes seen in epoch 62: 60
Label distribution in training epoch: Counter({1: 50, 0: 43})

Validation loss did not improve. Patience: 61/80

Epoch 63/100
Train Loss: 0.0149 | Train Acc: 98.92%
Val Loss: 2.2546 | Val Acc: 43.75%
Precision: 0.4000 | Recall: 0.4000 | F1 Score: 0.4000
Current AMP scale: 16384.0
Unique augmented volumes seen in epoch 63: 61
Label distribution in training epoch: Counter({1: 51, 0: 42})

Validation loss did not improve. Patience: 62/80

Epoch 64/100
Train Loss: 0.0358 | Train Acc: 97.85%
Val Loss: 5.8296 | Val Acc: 53.12%
Precision: 0.5593 | Recall: 0.5312 | F1 Score: 0.4684
Current AMP scale: 16384.0
Unique augmented volumes seen in epoch 64: 62
Label distribution in training epoch: Counter({1: 50, 0: 43})

Validation loss did not improve. Patience: 63/80

Epoch 65/100
Train Loss: 0.0093 | Train Acc: 100.00%
Val Loss: 2.8694 | Val Acc: 71.88%
Precision: 0.7196 | Recall: 0.7188 | F1 Score: 0.7185
Current AMP scale: 16384.0
Unique augmented volumes seen in epoch 65: 58
Label distribution in training epoch: Counter({1: 47, 0: 46})

Validation loss did not improve. Patience: 64/80

Epoch 66/100
Train Loss: 0.0021 | Train Acc: 100.00%
Val Loss: 3.7489 | Val Acc: 56.25%
Precision: 0.5455 | Recall: 0.5397 | F1 Score: 0.5333
Current AMP scale: 16384.0
Unique augmented volumes seen in epoch 66: 56
Label distribution in training epoch: Counter({0: 50, 1: 43})

Validation loss did not improve. Patience: 65/80

Epoch 67/100
Train Loss: 0.0015 | Train Acc: 100.00%
Val Loss: 3.4685 | Val Acc: 62.50%
Precision: 0.5543 | Recall: 0.5411 | F1 Score: 0.5362
Current AMP scale: 16384.0
Unique augmented volumes seen in epoch 67: 55
Label distribution in training epoch: Counter({0: 56, 1: 37})

Validation loss did not improve. Patience: 66/80

Epoch 68/100
Train Loss: 0.0531 | Train Acc: 97.85%
Val Loss: 5.5050 | Val Acc: 40.62%
Precision: 0.4375 | Recall: 0.4514 | F1 Score: 0.3914
Current AMP scale: 16384.0
Unique augmented volumes seen in epoch 68: 61
Label distribution in training epoch: Counter({0: 49, 1: 44})

Validation loss did not improve. Patience: 67/80

Epoch 69/100
Train Loss: 0.0659 | Train Acc: 97.85%
Val Loss: 4.6358 | Val Acc: 50.00%
Precision: 0.5417 | Recall: 0.5317 | F1 Score: 0.4818
Current AMP scale: 16384.0
Unique augmented volumes seen in epoch 69: 58
Label distribution in training epoch: Counter({0: 48, 1: 45})

Validation loss did not improve. Patience: 68/80

Epoch 70/100
Train Loss: 0.0289 | Train Acc: 98.92%
Val Loss: 2.9784 | Val Acc: 68.75%
Precision: 0.6812 | Recall: 0.6518 | F1 Score: 0.6537
Current AMP scale: 16384.0
Unique augmented volumes seen in epoch 70: 61
Label distribution in training epoch: Counter({1: 50, 0: 43})

Validation loss did not improve. Patience: 69/80

Epoch 71/100
Train Loss: 0.0403 | Train Acc: 98.92%
Val Loss: 2.9516 | Val Acc: 34.38%
Precision: 0.3259 | Recall: 0.3294 | F1 Score: 0.3273
Current AMP scale: 16384.0
Unique augmented volumes seen in epoch 71: 58
Label distribution in training epoch: Counter({1: 48, 0: 45})

Validation loss did not improve. Patience: 70/80

Epoch 72/100
Train Loss: 0.0107 | Train Acc: 100.00%
Val Loss: 4.2993 | Val Acc: 50.00%
Precision: 0.5079 | Recall: 0.5079 | F1 Score: 0.5000
Current AMP scale: 16384.0
Unique augmented volumes seen in epoch 72: 60
Label distribution in training epoch: Counter({1: 59, 0: 34})

Validation loss did not improve. Patience: 71/80

Epoch 73/100
Train Loss: 0.0016 | Train Acc: 100.00%
Val Loss: 3.7088 | Val Acc: 50.00%
Precision: 0.5108 | Recall: 0.5098 | F1 Score: 0.4921
Current AMP scale: 16384.0
Unique augmented volumes seen in epoch 73: 59
Label distribution in training epoch: Counter({1: 53, 0: 40})

Validation loss did not improve. Patience: 72/80

Epoch 74/100
Train Loss: 0.0008 | Train Acc: 100.00%
Val Loss: 3.2827 | Val Acc: 46.88%
Precision: 0.4583 | Recall: 0.4608 | F1 Score: 0.4555
Current AMP scale: 16384.0
Unique augmented volumes seen in epoch 74: 59
Label distribution in training epoch: Counter({1: 50, 0: 43})

Validation loss did not improve. Patience: 73/80

Epoch 75/100
Train Loss: 0.0019 | Train Acc: 100.00%
Val Loss: 3.6800 | Val Acc: 43.75%
Precision: 0.5417 | Recall: 0.5364 | F1 Score: 0.4353
Current AMP scale: 16384.0
Unique augmented volumes seen in epoch 75: 59
Label distribution in training epoch: Counter({1: 48, 0: 45})

Validation loss did not improve. Patience: 74/80

Epoch 76/100
Train Loss: 0.0012 | Train Acc: 100.00%
Val Loss: 3.8816 | Val Acc: 43.75%
Precision: 0.4275 | Recall: 0.4199 | F1 Score: 0.4170
Current AMP scale: 16384.0
Unique augmented volumes seen in epoch 76: 61
Label distribution in training epoch: Counter({0: 54, 1: 39})

Validation loss did not improve. Patience: 75/80

Epoch 77/100
Train Loss: 0.0006 | Train Acc: 100.00%
Val Loss: 3.1448 | Val Acc: 56.25%
Precision: 0.5625 | Recall: 0.5635 | F1 Score: 0.5608
Current AMP scale: 16384.0
Unique augmented volumes seen in epoch 77: 59
Label distribution in training epoch: Counter({0: 47, 1: 46})

Validation loss did not improve. Patience: 76/80

Epoch 78/100
Train Loss: 0.0256 | Train Acc: 98.92%
Val Loss: 2.8341 | Val Acc: 50.00%
Precision: 0.5000 | Recall: 0.5000 | F1 Score: 0.4667
Current AMP scale: 16384.0
Unique augmented volumes seen in epoch 78: 59
Label distribution in training epoch: Counter({0: 47, 1: 46})

Validation loss did not improve. Patience: 77/80

Epoch 79/100
Train Loss: 0.0007 | Train Acc: 100.00%
Val Loss: 2.6290 | Val Acc: 28.12%
Precision: 0.2897 | Recall: 0.2854 | F1 Score: 0.2805
Current AMP scale: 16384.0
Unique augmented volumes seen in epoch 79: 62
Label distribution in training epoch: Counter({0: 48, 1: 45})

Validation loss did not improve. Patience: 78/80

Epoch 80/100
Train Loss: 0.0005 | Train Acc: 100.00%
Val Loss: 2.5160 | Val Acc: 53.12%
Precision: 0.5198 | Recall: 0.5202 | F1 Score: 0.5195
Current AMP scale: 16384.0
Unique augmented volumes seen in epoch 80: 57
Label distribution in training epoch: Counter({1: 50, 0: 43})

Validation loss did not improve. Patience: 79/80

Epoch 81/100
Train Loss: 0.0002 | Train Acc: 100.00%
Val Loss: 2.5091 | Val Acc: 50.00%
Precision: 0.5507 | Recall: 0.5425 | F1 Score: 0.4921
Current AMP scale: 16384.0
Unique augmented volumes seen in epoch 81: 56
Label distribution in training epoch: Counter({0: 47, 1: 46})

Validation loss did not improve. Patience: 80/80

Early stopping triggered after 81 epochs.


Training complete.
Loading best model from /home/etudiant/Projets/Viviane/LIDC-ML/models/best_model_resnet_pytorch3D_architecture_v0.pth for final metrics.
######## Training Finished in 0h 58m 50s ###########
Test Accuracy on 32 images: 62.50%
AUC: 0.6623
AUC: 0.5982
Class 0-non-cancer: Precision: 0.36, Recall: 0.29, F1-Score: 0.32
Class 1-cancer: Precision: 0.52, Recall: 0.61, F1-Score: 0.56
