

==== Training started at 2025-07-20 22:15:48.287535 ====

Using Gradient Accumulation with 4 steps.
DataLoader batch size: 8
Effective batch size: 32

Epoch 1/150
Train Loss: 0.6829 | Train Acc: 50.54%
Val Loss: 0.6694 | Val Acc: 56.25%
Precision: 0.2812 | Recall: 0.5000 | F1 Score: 0.3600
Current AMP scale: 65536.0
Unique augmented volumes seen in epoch 1: 54
Label distribution in training epoch: Counter({1: 48, 0: 45})

Validation loss improved. Saving best model to /home/etudiant/Projets/Viviane/LIDC-ML/models/best_model_resnet_pytorch3D_architecture_v0.pth

Epoch 2/150
Train Loss: 0.6700 | Train Acc: 58.06%
Val Loss: 0.6527 | Val Acc: 59.38%
Precision: 0.6607 | Recall: 0.5706 | F1 Score: 0.5135
Current AMP scale: 65536.0
Unique augmented volumes seen in epoch 2: 62
Label distribution in training epoch: Counter({1: 51, 0: 42})

Validation loss improved. Saving best model to /home/etudiant/Projets/Viviane/LIDC-ML/models/best_model_resnet_pytorch3D_architecture_v0.pth

Epoch 3/150
Train Loss: 0.6045 | Train Acc: 67.74%
Val Loss: 0.7035 | Val Acc: 53.12%
Precision: 0.5202 | Recall: 0.5198 | F1 Score: 0.5195
Current AMP scale: 65536.0
Unique augmented volumes seen in epoch 3: 60
Label distribution in training epoch: Counter({0: 47, 1: 46})

Validation loss did not improve. Patience: 1/120

Epoch 4/150
Train Loss: 0.6012 | Train Acc: 69.89%
Val Loss: 0.7002 | Val Acc: 50.00%
Precision: 0.5000 | Recall: 0.5000 | F1 Score: 0.4921
Current AMP scale: 65536.0
Unique augmented volumes seen in epoch 4: 67
Label distribution in training epoch: Counter({0: 49, 1: 44})

Validation loss did not improve. Patience: 2/120

Epoch 5/150
Train Loss: 0.6055 | Train Acc: 66.67%
Val Loss: 0.7773 | Val Acc: 53.12%
Precision: 0.5353 | Recall: 0.5357 | F1 Score: 0.5308
Current AMP scale: 65536.0
Unique augmented volumes seen in epoch 5: 54
Label distribution in training epoch: Counter({0: 54, 1: 39})

Validation loss did not improve. Patience: 3/120

Epoch 6/150
Train Loss: 0.5715 | Train Acc: 73.12%
Val Loss: 0.6179 | Val Acc: 71.88%
Precision: 0.7188 | Recall: 0.7196 | F1 Score: 0.7185
Current AMP scale: 65536.0
Unique augmented volumes seen in epoch 6: 60
Label distribution in training epoch: Counter({1: 59, 0: 34})

Validation loss improved. Saving best model to /home/etudiant/Projets/Viviane/LIDC-ML/models/best_model_resnet_pytorch3D_architecture_v0.pth

Epoch 7/150
Train Loss: 0.6171 | Train Acc: 63.44%
Val Loss: 0.7644 | Val Acc: 59.38%
Precision: 0.5938 | Recall: 0.5972 | F1 Score: 0.5901
Current AMP scale: 65536.0
Unique augmented volumes seen in epoch 7: 59
Label distribution in training epoch: Counter({1: 50, 0: 43})

Validation loss did not improve. Patience: 1/120

Epoch 8/150
Train Loss: 0.4711 | Train Acc: 87.10%
Val Loss: 0.6776 | Val Acc: 62.50%
Precision: 0.6196 | Recall: 0.6235 | F1 Score: 0.6190
Current AMP scale: 65536.0
Unique augmented volumes seen in epoch 8: 60
Label distribution in training epoch: Counter({0: 51, 1: 42})

Validation loss did not improve. Patience: 2/120

Epoch 9/150
Train Loss: 0.4798 | Train Acc: 76.34%
Val Loss: 0.8359 | Val Acc: 59.38%
Precision: 0.5941 | Recall: 0.5938 | F1 Score: 0.5934
Current AMP scale: 65536.0
Unique augmented volumes seen in epoch 9: 62
Label distribution in training epoch: Counter({1: 54, 0: 39})

Validation loss did not improve. Patience: 3/120

Epoch 10/150
Train Loss: 0.4508 | Train Acc: 76.34%
Val Loss: 0.9315 | Val Acc: 56.25%
Precision: 0.6280 | Recall: 0.6073 | F1 Score: 0.5556
Current AMP scale: 65536.0
Unique augmented volumes seen in epoch 10: 58
Label distribution in training epoch: Counter({0: 51, 1: 42})

Validation loss did not improve. Patience: 4/120

Epoch 11/150
Train Loss: 0.3572 | Train Acc: 87.10%
Val Loss: 1.0423 | Val Acc: 46.88%
Precision: 0.4500 | Recall: 0.4569 | F1 Score: 0.4421
Current AMP scale: 65536.0
Unique augmented volumes seen in epoch 11: 57
Label distribution in training epoch: Counter({0: 50, 1: 43})

Validation loss did not improve. Patience: 5/120

Epoch 12/150
Train Loss: 0.3143 | Train Acc: 90.32%
Val Loss: 0.9868 | Val Acc: 43.75%
Precision: 0.4413 | Recall: 0.4431 | F1 Score: 0.4353
Current AMP scale: 65536.0
Unique augmented volumes seen in epoch 12: 55
Label distribution in training epoch: Counter({0: 53, 1: 40})

Validation loss did not improve. Patience: 6/120

Epoch 13/150
Train Loss: 0.2978 | Train Acc: 87.10%
Val Loss: 0.9747 | Val Acc: 40.62%
Precision: 0.4062 | Recall: 0.4059 | F1 Score: 0.4057
Current AMP scale: 65536.0
Unique augmented volumes seen in epoch 13: 58
Label distribution in training epoch: Counter({0: 51, 1: 42})

Validation loss did not improve. Patience: 7/120

Epoch 14/150
Train Loss: 0.2918 | Train Acc: 87.10%
Val Loss: 1.3220 | Val Acc: 37.50%
Precision: 0.3750 | Recall: 0.3545 | F1 Score: 0.3522
Current AMP scale: 65536.0
Unique augmented volumes seen in epoch 14: 59
Label distribution in training epoch: Counter({1: 51, 0: 42})

Validation loss did not improve. Patience: 8/120

Epoch 15/150
Train Loss: 0.2671 | Train Acc: 91.40%
Val Loss: 0.8119 | Val Acc: 56.25%
Precision: 0.5833 | Recall: 0.5794 | F1 Score: 0.5608
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 15: 60
Label distribution in training epoch: Counter({1: 53, 0: 40})

Validation loss did not improve. Patience: 9/120

Epoch 16/150
Train Loss: 0.2375 | Train Acc: 90.32%
Val Loss: 1.0604 | Val Acc: 40.62%
Precision: 0.4167 | Recall: 0.4150 | F1 Score: 0.4057
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 16: 50
Label distribution in training epoch: Counter({0: 61, 1: 32})

Validation loss did not improve. Patience: 10/120

Epoch 17/150
Train Loss: 0.1852 | Train Acc: 91.40%
Val Loss: 1.1530 | Val Acc: 40.62%
Precision: 0.3958 | Recall: 0.4216 | F1 Score: 0.3764
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 17: 58
Label distribution in training epoch: Counter({1: 48, 0: 45})

Validation loss did not improve. Patience: 11/120

Epoch 18/150
Train Loss: 0.1592 | Train Acc: 94.62%
Val Loss: 1.6605 | Val Acc: 40.62%
Precision: 0.4087 | Recall: 0.4098 | F1 Score: 0.4057
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 18: 58
Label distribution in training epoch: Counter({0: 54, 1: 39})

Validation loss did not improve. Patience: 12/120

Epoch 19/150
Train Loss: 0.1071 | Train Acc: 96.77%
Val Loss: 1.4923 | Val Acc: 56.25%
Precision: 0.5466 | Recall: 0.5466 | F1 Score: 0.5466
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 19: 54
Label distribution in training epoch: Counter({0: 49, 1: 44})

Validation loss did not improve. Patience: 13/120

Epoch 20/150
Train Loss: 0.0747 | Train Acc: 98.92%
Val Loss: 1.4474 | Val Acc: 37.50%
Precision: 0.3667 | Recall: 0.3750 | F1 Score: 0.3651
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 20: 54
Label distribution in training epoch: Counter({0: 56, 1: 37})

Validation loss did not improve. Patience: 14/120

Epoch 21/150
Train Loss: 0.1630 | Train Acc: 95.70%
Val Loss: 1.7450 | Val Acc: 43.75%
Precision: 0.4583 | Recall: 0.4683 | F1 Score: 0.4170
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 21: 62
Label distribution in training epoch: Counter({1: 53, 0: 40})

Validation loss did not improve. Patience: 15/120

Epoch 22/150
Train Loss: 0.0414 | Train Acc: 100.00%
Val Loss: 1.6225 | Val Acc: 50.00%
Precision: 0.5000 | Recall: 0.5000 | F1 Score: 0.4980
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 22: 53
Label distribution in training epoch: Counter({0: 51, 1: 42})

Validation loss did not improve. Patience: 16/120

Epoch 23/150
Train Loss: 0.0417 | Train Acc: 98.92%
Val Loss: 1.6229 | Val Acc: 46.88%
Precision: 0.4643 | Recall: 0.4647 | F1 Score: 0.4640
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 23: 59
Label distribution in training epoch: Counter({0: 52, 1: 41})

Validation loss did not improve. Patience: 17/120

Epoch 24/150
Train Loss: 0.0503 | Train Acc: 98.92%
Val Loss: 1.5469 | Val Acc: 53.12%
Precision: 0.5357 | Recall: 0.5353 | F1 Score: 0.5308
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 24: 61
Label distribution in training epoch: Counter({1: 47, 0: 46})

Validation loss did not improve. Patience: 18/120

Epoch 25/150
Train Loss: 0.0120 | Train Acc: 100.00%
Val Loss: 1.6846 | Val Acc: 46.88%
Precision: 0.4750 | Recall: 0.4765 | F1 Score: 0.4640
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 25: 56
Label distribution in training epoch: Counter({0: 52, 1: 41})

Validation loss did not improve. Patience: 19/120

Epoch 26/150
Train Loss: 0.0900 | Train Acc: 94.62%
Val Loss: 2.3023 | Val Acc: 31.25%
Precision: 0.3500 | Recall: 0.3364 | F1 Score: 0.3098
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 26: 60
Label distribution in training epoch: Counter({1: 52, 0: 41})

Validation loss did not improve. Patience: 20/120

Epoch 27/150
Train Loss: 0.0420 | Train Acc: 98.92%
Val Loss: 2.1683 | Val Acc: 43.75%
Precision: 0.4333 | Recall: 0.4375 | F1 Score: 0.4286
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 27: 56
Label distribution in training epoch: Counter({1: 47, 0: 46})

Validation loss did not improve. Patience: 21/120

Epoch 28/150
Train Loss: 0.0411 | Train Acc: 98.92%
Val Loss: 1.8347 | Val Acc: 50.00%
Precision: 0.5059 | Recall: 0.5061 | F1 Score: 0.4980
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 28: 61
Label distribution in training epoch: Counter({1: 49, 0: 44})

Validation loss did not improve. Patience: 22/120

Epoch 29/150
Train Loss: 0.1514 | Train Acc: 93.55%
Val Loss: 1.8629 | Val Acc: 56.25%
Precision: 0.5569 | Recall: 0.5587 | F1 Score: 0.5556
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 29: 57
Label distribution in training epoch: Counter({0: 54, 1: 39})

Validation loss did not improve. Patience: 23/120

Epoch 30/150
Train Loss: 0.0125 | Train Acc: 100.00%
Val Loss: 1.6339 | Val Acc: 50.00%
Precision: 0.5000 | Recall: 0.5000 | F1 Score: 0.4818
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 30: 56
Label distribution in training epoch: Counter({1: 52, 0: 41})

Validation loss did not improve. Patience: 24/120

Epoch 31/150
Train Loss: 0.0616 | Train Acc: 97.85%
Val Loss: 2.2982 | Val Acc: 43.75%
Precision: 0.4392 | Recall: 0.4392 | F1 Score: 0.4375
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 31: 55
Label distribution in training epoch: Counter({0: 51, 1: 42})

Validation loss did not improve. Patience: 25/120

Epoch 32/150
Train Loss: 0.0120 | Train Acc: 100.00%
Val Loss: 1.4877 | Val Acc: 56.25%
Precision: 0.5952 | Recall: 0.6250 | F1 Score: 0.5466
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 32: 60
Label distribution in training epoch: Counter({1: 50, 0: 43})

Validation loss did not improve. Patience: 26/120

Epoch 33/150
Train Loss: 0.0186 | Train Acc: 100.00%
Val Loss: 1.9359 | Val Acc: 40.62%
Precision: 0.4083 | Recall: 0.4137 | F1 Score: 0.4010
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 33: 62
Label distribution in training epoch: Counter({1: 52, 0: 41})

Validation loss did not improve. Patience: 27/120

Epoch 34/150
Train Loss: 0.0084 | Train Acc: 100.00%
Val Loss: 1.6385 | Val Acc: 62.50%
Precision: 0.6349 | Recall: 0.6349 | F1 Score: 0.6250
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 34: 58
Label distribution in training epoch: Counter({1: 47, 0: 46})

Validation loss did not improve. Patience: 28/120

Epoch 35/150
Train Loss: 0.0253 | Train Acc: 98.92%
Val Loss: 2.1926 | Val Acc: 43.75%
Precision: 0.4199 | Recall: 0.4275 | F1 Score: 0.4170
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 35: 58
Label distribution in training epoch: Counter({1: 49, 0: 44})

Validation loss did not improve. Patience: 29/120

Epoch 36/150
Train Loss: 0.1496 | Train Acc: 94.62%
Val Loss: 2.7218 | Val Acc: 50.00%
Precision: 0.5257 | Recall: 0.5176 | F1 Score: 0.4667
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 36: 61
Label distribution in training epoch: Counter({1: 48, 0: 45})

Validation loss did not improve. Patience: 30/120

Epoch 37/150
Train Loss: 0.0196 | Train Acc: 98.92%
Val Loss: 2.1728 | Val Acc: 56.25%
Precision: 0.5709 | Recall: 0.5686 | F1 Score: 0.5608
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 37: 60
Label distribution in training epoch: Counter({1: 51, 0: 42})

Validation loss did not improve. Patience: 31/120

Epoch 38/150
Train Loss: 0.3311 | Train Acc: 90.32%
Val Loss: 1.9092 | Val Acc: 43.75%
Precision: 0.4314 | Recall: 0.4291 | F1 Score: 0.4286
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 38: 58
Label distribution in training epoch: Counter({0: 57, 1: 36})

Validation loss did not improve. Patience: 32/120

Epoch 39/150
Train Loss: 0.0259 | Train Acc: 100.00%
Val Loss: 2.3804 | Val Acc: 53.12%
Precision: 0.5346 | Recall: 0.5312 | F1 Score: 0.5195
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 39: 60
Label distribution in training epoch: Counter({1: 52, 0: 41})

Validation loss did not improve. Patience: 33/120

Epoch 40/150
Train Loss: 0.0249 | Train Acc: 100.00%
Val Loss: 1.8356 | Val Acc: 50.00%
Precision: 0.4921 | Recall: 0.4921 | F1 Score: 0.4921
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 40: 52
Label distribution in training epoch: Counter({1: 54, 0: 39})

Validation loss did not improve. Patience: 34/120

Epoch 41/150
Train Loss: 0.0583 | Train Acc: 97.85%
Val Loss: 3.2604 | Val Acc: 37.50%
Precision: 0.3887 | Recall: 0.3887 | F1 Score: 0.3750
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 41: 53
Label distribution in training epoch: Counter({0: 54, 1: 39})

Validation loss did not improve. Patience: 35/120

Epoch 42/150
Train Loss: 0.0529 | Train Acc: 98.92%
Val Loss: 2.1807 | Val Acc: 62.50%
Precision: 0.6182 | Recall: 0.6032 | F1 Score: 0.6000
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 42: 58
Label distribution in training epoch: Counter({0: 49, 1: 44})

Validation loss did not improve. Patience: 36/120

Epoch 43/150
Train Loss: 0.0067 | Train Acc: 100.00%
Val Loss: 1.7826 | Val Acc: 56.25%
Precision: 0.5625 | Recall: 0.5667 | F1 Score: 0.5556
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 43: 61
Label distribution in training epoch: Counter({0: 50, 1: 43})

Validation loss did not improve. Patience: 37/120

Epoch 44/150
Train Loss: 0.0036 | Train Acc: 100.00%
Val Loss: 2.5894 | Val Acc: 34.38%
Precision: 0.3623 | Recall: 0.3583 | F1 Score: 0.3431
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 44: 52
Label distribution in training epoch: Counter({1: 52, 0: 41})

Validation loss did not improve. Patience: 38/120

Epoch 45/150
Train Loss: 0.2627 | Train Acc: 95.70%
Val Loss: 2.6952 | Val Acc: 56.25%
Precision: 0.5714 | Recall: 0.5714 | F1 Score: 0.5625
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 45: 58
Label distribution in training epoch: Counter({0: 52, 1: 41})

Validation loss did not improve. Patience: 39/120

Epoch 46/150
Train Loss: 0.0030 | Train Acc: 100.00%
Val Loss: 2.3157 | Val Acc: 53.12%
Precision: 0.5392 | Recall: 0.5417 | F1 Score: 0.5271
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 46: 56
Label distribution in training epoch: Counter({0: 49, 1: 44})

Validation loss did not improve. Patience: 40/120

Epoch 47/150
Train Loss: 0.1274 | Train Acc: 96.77%
Val Loss: 1.5630 | Val Acc: 53.12%
Precision: 0.5725 | Recall: 0.5595 | F1 Score: 0.5195
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 47: 55
Label distribution in training epoch: Counter({0: 51, 1: 42})

Validation loss did not improve. Patience: 41/120

Epoch 48/150
Train Loss: 0.0196 | Train Acc: 100.00%
Val Loss: 2.7538 | Val Acc: 34.38%
Precision: 0.3373 | Recall: 0.3392 | F1 Score: 0.3379
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 48: 61
Label distribution in training epoch: Counter({0: 48, 1: 45})

Validation loss did not improve. Patience: 42/120

Epoch 49/150
Train Loss: 0.0072 | Train Acc: 100.00%
Val Loss: 2.7029 | Val Acc: 40.62%
Precision: 0.4246 | Recall: 0.4177 | F1 Score: 0.4010
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 49: 61
Label distribution in training epoch: Counter({1: 47, 0: 46})

Validation loss did not improve. Patience: 43/120

Epoch 50/150
Train Loss: 0.0247 | Train Acc: 98.92%
Val Loss: 2.0653 | Val Acc: 31.25%
Precision: 0.3098 | Recall: 0.3098 | F1 Score: 0.3098
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 50: 64
Label distribution in training epoch: Counter({1: 47, 0: 46})

Validation loss did not improve. Patience: 44/120

Epoch 51/150
Train Loss: 0.0172 | Train Acc: 100.00%
Val Loss: 1.7300 | Val Acc: 43.75%
Precision: 0.4534 | Recall: 0.4534 | F1 Score: 0.4375
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 51: 60
Label distribution in training epoch: Counter({0: 51, 1: 42})

Validation loss did not improve. Patience: 45/120

Epoch 52/150
Train Loss: 0.0686 | Train Acc: 97.85%
Val Loss: 1.8374 | Val Acc: 53.12%
Precision: 0.5437 | Recall: 0.5445 | F1 Score: 0.5308
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 52: 62
Label distribution in training epoch: Counter({0: 50, 1: 43})

Validation loss did not improve. Patience: 46/120

Epoch 53/150
Train Loss: 0.0574 | Train Acc: 98.92%
Val Loss: 1.6068 | Val Acc: 56.25%
Precision: 0.5455 | Recall: 0.5397 | F1 Score: 0.5333
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 53: 67
Label distribution in training epoch: Counter({1: 52, 0: 41})

Validation loss did not improve. Patience: 47/120

Epoch 54/150
Train Loss: 0.0166 | Train Acc: 98.92%
Val Loss: 1.4155 | Val Acc: 62.50%
Precision: 0.6410 | Recall: 0.5873 | F1 Score: 0.5636
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 54: 57
Label distribution in training epoch: Counter({0: 48, 1: 45})

Validation loss did not improve. Patience: 48/120

Epoch 55/150
Train Loss: 0.0325 | Train Acc: 98.92%
Val Loss: 2.3145 | Val Acc: 53.12%
Precision: 0.5445 | Recall: 0.5437 | F1 Score: 0.5308
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 55: 61
Label distribution in training epoch: Counter({1: 47, 0: 46})

Validation loss did not improve. Patience: 49/120

Epoch 56/150
Train Loss: 0.1450 | Train Acc: 95.70%
Val Loss: 4.4111 | Val Acc: 40.62%
Precision: 0.3958 | Recall: 0.4216 | F1 Score: 0.3764
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 56: 59
Label distribution in training epoch: Counter({1: 56, 0: 37})

Validation loss did not improve. Patience: 50/120

Epoch 57/150
Train Loss: 0.0673 | Train Acc: 97.85%
Val Loss: 4.3120 | Val Acc: 40.62%
Precision: 0.3785 | Recall: 0.3750 | F1 Score: 0.3764
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 57: 56
Label distribution in training epoch: Counter({0: 52, 1: 41})

Validation loss did not improve. Patience: 51/120

Epoch 58/150
Train Loss: 0.0292 | Train Acc: 98.92%
Val Loss: 2.3430 | Val Acc: 62.50%
Precision: 0.6196 | Recall: 0.6235 | F1 Score: 0.6190
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 58: 57
Label distribution in training epoch: Counter({0: 53, 1: 40})

Validation loss did not improve. Patience: 52/120

Epoch 59/150
Train Loss: 0.0302 | Train Acc: 98.92%
Val Loss: 2.1736 | Val Acc: 43.75%
Precision: 0.4392 | Recall: 0.4392 | F1 Score: 0.4375
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 59: 66
Label distribution in training epoch: Counter({0: 47, 1: 46})

Validation loss did not improve. Patience: 53/120

Epoch 60/150
Train Loss: 0.0173 | Train Acc: 98.92%
Val Loss: 3.5614 | Val Acc: 31.25%
Precision: 0.3239 | Recall: 0.3239 | F1 Score: 0.3125
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 60: 53
Label distribution in training epoch: Counter({1: 48, 0: 45})

Validation loss did not improve. Patience: 54/120

Epoch 61/150
Train Loss: 0.0560 | Train Acc: 97.85%
Val Loss: 1.6610 | Val Acc: 62.50%
Precision: 0.6196 | Recall: 0.6235 | F1 Score: 0.6190
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 61: 52
Label distribution in training epoch: Counter({0: 54, 1: 39})

Validation loss did not improve. Patience: 55/120

Epoch 62/150
Train Loss: 0.1118 | Train Acc: 97.85%
Val Loss: 2.2528 | Val Acc: 46.88%
Precision: 0.4722 | Recall: 0.4725 | F1 Score: 0.4682
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 62: 60
Label distribution in training epoch: Counter({1: 50, 0: 43})

Validation loss did not improve. Patience: 56/120

Epoch 63/150
Train Loss: 0.0047 | Train Acc: 100.00%
Val Loss: 2.7574 | Val Acc: 50.00%
Precision: 0.5333 | Recall: 0.5333 | F1 Score: 0.5000
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 63: 61
Label distribution in training epoch: Counter({1: 51, 0: 42})

Validation loss did not improve. Patience: 57/120

Epoch 64/150
Train Loss: 0.0045 | Train Acc: 100.00%
Val Loss: 1.9262 | Val Acc: 46.88%
Precision: 0.4676 | Recall: 0.4688 | F1 Score: 0.4640
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 64: 62
Label distribution in training epoch: Counter({1: 50, 0: 43})

Validation loss did not improve. Patience: 58/120

Epoch 65/150
Train Loss: 0.0018 | Train Acc: 100.00%
Val Loss: 1.5485 | Val Acc: 56.25%
Precision: 0.5635 | Recall: 0.5625 | F1 Score: 0.5608
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 65: 58
Label distribution in training epoch: Counter({1: 47, 0: 46})

Validation loss did not improve. Patience: 59/120

Epoch 66/150
Train Loss: 0.0031 | Train Acc: 100.00%
Val Loss: 2.2790 | Val Acc: 50.00%
Precision: 0.4583 | Recall: 0.4683 | F1 Score: 0.4459
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 66: 56
Label distribution in training epoch: Counter({0: 50, 1: 43})

Validation loss did not improve. Patience: 60/120

Epoch 67/150
Train Loss: 0.0101 | Train Acc: 100.00%
Val Loss: 2.7859 | Val Acc: 40.62%
Precision: 0.3849 | Recall: 0.3745 | F1 Score: 0.3764
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 67: 55
Label distribution in training epoch: Counter({0: 56, 1: 37})

Validation loss did not improve. Patience: 61/120

Epoch 68/150
Train Loss: 0.0328 | Train Acc: 100.00%
Val Loss: 3.2354 | Val Acc: 43.75%
Precision: 0.4632 | Recall: 0.4656 | F1 Score: 0.4353
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 68: 61
Label distribution in training epoch: Counter({0: 49, 1: 44})

Validation loss did not improve. Patience: 62/120

Epoch 69/150
Train Loss: 0.0057 | Train Acc: 100.00%
Val Loss: 2.0992 | Val Acc: 65.62%
Precision: 0.6515 | Recall: 0.6389 | F1 Score: 0.6390
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 69: 58
Label distribution in training epoch: Counter({0: 48, 1: 45})

Validation loss did not improve. Patience: 63/120

Epoch 70/150
Train Loss: 0.0131 | Train Acc: 100.00%
Val Loss: 3.6706 | Val Acc: 37.50%
Precision: 0.3939 | Recall: 0.4008 | F1 Score: 0.3725
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 70: 61
Label distribution in training epoch: Counter({1: 50, 0: 43})

Validation loss did not improve. Patience: 64/120

Epoch 71/150
Train Loss: 0.0077 | Train Acc: 100.00%
Val Loss: 2.0026 | Val Acc: 59.38%
Precision: 0.5980 | Recall: 0.5992 | F1 Score: 0.5934
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 71: 58
Label distribution in training epoch: Counter({1: 48, 0: 45})

Validation loss did not improve. Patience: 65/120

Epoch 72/150
Train Loss: 0.0046 | Train Acc: 100.00%
Val Loss: 1.8911 | Val Acc: 65.62%
Precision: 0.6741 | Recall: 0.6706 | F1 Score: 0.6559
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 72: 60
Label distribution in training epoch: Counter({1: 59, 0: 34})

Validation loss did not improve. Patience: 66/120

Epoch 73/150
Train Loss: 0.0253 | Train Acc: 98.92%
Val Loss: 2.0490 | Val Acc: 56.25%
Precision: 0.5584 | Recall: 0.5529 | F1 Score: 0.5466
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 73: 59
Label distribution in training epoch: Counter({1: 53, 0: 40})

Validation loss did not improve. Patience: 67/120

Epoch 74/150
Train Loss: 0.0022 | Train Acc: 100.00%
Val Loss: 2.4175 | Val Acc: 46.88%
Precision: 0.4722 | Recall: 0.4725 | F1 Score: 0.4682
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 74: 59
Label distribution in training epoch: Counter({1: 50, 0: 43})

Validation loss did not improve. Patience: 68/120

Epoch 75/150
Train Loss: 0.0044 | Train Acc: 100.00%
Val Loss: 2.7610 | Val Acc: 53.12%
Precision: 0.5996 | Recall: 0.6045 | F1 Score: 0.5308
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 75: 59
Label distribution in training epoch: Counter({1: 48, 0: 45})

Validation loss did not improve. Patience: 69/120

Epoch 76/150
Train Loss: 0.0025 | Train Acc: 100.00%
Val Loss: 3.6209 | Val Acc: 56.25%
Precision: 0.5951 | Recall: 0.6017 | F1 Score: 0.5608
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 76: 61
Label distribution in training epoch: Counter({0: 54, 1: 39})

Validation loss did not improve. Patience: 70/120

Epoch 77/150
Train Loss: 0.2435 | Train Acc: 90.32%
Val Loss: 3.0248 | Val Acc: 46.88%
Precision: 0.4798 | Recall: 0.4802 | F1 Score: 0.4682
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 77: 59
Label distribution in training epoch: Counter({0: 47, 1: 46})

Validation loss did not improve. Patience: 71/120

Epoch 78/150
Train Loss: 0.0031 | Train Acc: 100.00%
Val Loss: 2.1157 | Val Acc: 50.00%
Precision: 0.5000 | Recall: 0.5000 | F1 Score: 0.4667
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 78: 59
Label distribution in training epoch: Counter({0: 47, 1: 46})

Validation loss did not improve. Patience: 72/120

Epoch 79/150
Train Loss: 0.0036 | Train Acc: 100.00%
Val Loss: 3.2381 | Val Acc: 34.38%
Precision: 0.3583 | Recall: 0.3623 | F1 Score: 0.3431
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 79: 62
Label distribution in training epoch: Counter({0: 48, 1: 45})

Validation loss did not improve. Patience: 73/120

Epoch 80/150
Train Loss: 0.0117 | Train Acc: 100.00%
Val Loss: 2.7535 | Val Acc: 40.62%
Precision: 0.3750 | Recall: 0.3785 | F1 Score: 0.3764
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 80: 57
Label distribution in training epoch: Counter({1: 50, 0: 43})

Validation loss did not improve. Patience: 74/120

Epoch 81/150
Train Loss: 0.0014 | Train Acc: 100.00%
Val Loss: 2.8701 | Val Acc: 50.00%
Precision: 0.5507 | Recall: 0.5425 | F1 Score: 0.4921
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 81: 56
Label distribution in training epoch: Counter({0: 47, 1: 46})

Validation loss did not improve. Patience: 75/120

Epoch 82/150
Train Loss: 0.0677 | Train Acc: 96.77%
Val Loss: 3.0400 | Val Acc: 46.88%
Precision: 0.4583 | Recall: 0.4608 | F1 Score: 0.4555
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 82: 54
Label distribution in training epoch: Counter({0: 58, 1: 35})

Validation loss did not improve. Patience: 76/120

Epoch 83/150
Train Loss: 0.0389 | Train Acc: 97.85%
Val Loss: 2.8309 | Val Acc: 56.25%
Precision: 0.5556 | Recall: 0.5556 | F1 Score: 0.5556
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 83: 57
Label distribution in training epoch: Counter({1: 54, 0: 39})

Validation loss did not improve. Patience: 77/120

Epoch 84/150
Train Loss: 0.0017 | Train Acc: 100.00%
Val Loss: 2.0063 | Val Acc: 56.25%
Precision: 0.5587 | Recall: 0.5569 | F1 Score: 0.5556
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 84: 50
Label distribution in training epoch: Counter({0: 55, 1: 38})

Validation loss did not improve. Patience: 78/120

Epoch 85/150
Train Loss: 0.0304 | Train Acc: 98.92%
Val Loss: 3.1736 | Val Acc: 43.75%
Precision: 0.4534 | Recall: 0.4534 | F1 Score: 0.4375
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 85: 63
Label distribution in training epoch: Counter({1: 52, 0: 41})

Validation loss did not improve. Patience: 79/120

Epoch 86/150
Train Loss: 0.0041 | Train Acc: 100.00%
Val Loss: 4.1327 | Val Acc: 28.12%
Precision: 0.2591 | Recall: 0.2922 | F1 Score: 0.2633
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 86: 57
Label distribution in training epoch: Counter({1: 53, 0: 40})

Validation loss did not improve. Patience: 80/120

Epoch 87/150
Train Loss: 0.0008 | Train Acc: 100.00%
Val Loss: 2.3711 | Val Acc: 56.25%
Precision: 0.4629 | Recall: 0.4719 | F1 Score: 0.4589
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 87: 63
Label distribution in training epoch: Counter({1: 49, 0: 44})

Validation loss did not improve. Patience: 81/120

Epoch 88/150
Train Loss: 0.0044 | Train Acc: 100.00%
Val Loss: 3.0084 | Val Acc: 46.88%
Precision: 0.4792 | Recall: 0.4843 | F1 Score: 0.4421
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 88: 55
Label distribution in training epoch: Counter({0: 53, 1: 40})

Validation loss did not improve. Patience: 82/120

Epoch 89/150
Train Loss: 0.1367 | Train Acc: 97.85%
Val Loss: 2.0587 | Val Acc: 50.00%
Precision: 0.4833 | Recall: 0.4841 | F1 Score: 0.4818
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 89: 58
Label distribution in training epoch: Counter({1: 50, 0: 43})

Validation loss did not improve. Patience: 83/120

Epoch 90/150
Train Loss: 0.0010 | Train Acc: 100.00%
Val Loss: 4.9694 | Val Acc: 34.38%
Precision: 0.3864 | Recall: 0.3918 | F1 Score: 0.3431
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 90: 57
Label distribution in training epoch: Counter({1: 50, 0: 43})

Validation loss did not improve. Patience: 84/120

Epoch 91/150
Train Loss: 0.0009 | Train Acc: 100.00%
Val Loss: 2.5022 | Val Acc: 56.25%
Precision: 0.5625 | Recall: 0.5625 | F1 Score: 0.5625
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 91: 58
Label distribution in training epoch: Counter({0: 51, 1: 42})

Validation loss did not improve. Patience: 85/120

Epoch 92/150
Train Loss: 0.0025 | Train Acc: 100.00%
Val Loss: 3.5429 | Val Acc: 53.12%
Precision: 0.5314 | Recall: 0.5312 | F1 Score: 0.5308
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 92: 58
Label distribution in training epoch: Counter({1: 50, 0: 43})

Validation loss did not improve. Patience: 86/120

Epoch 93/150
Train Loss: 0.0722 | Train Acc: 97.85%
Val Loss: 1.7500 | Val Acc: 62.50%
Precision: 0.6167 | Recall: 0.6111 | F1 Score: 0.6113
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 93: 58
Label distribution in training epoch: Counter({0: 50, 1: 43})

Validation loss did not improve. Patience: 87/120

Epoch 94/150
Train Loss: 0.0046 | Train Acc: 100.00%
Val Loss: 2.2401 | Val Acc: 46.88%
Precision: 0.4952 | Recall: 0.4960 | F1 Score: 0.4555
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 94: 62
Label distribution in training epoch: Counter({1: 47, 0: 46})

Validation loss did not improve. Patience: 88/120

Epoch 95/150
Train Loss: 0.0037 | Train Acc: 100.00%
Val Loss: 1.4631 | Val Acc: 71.88%
Precision: 0.7188 | Recall: 0.7196 | F1 Score: 0.7185
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 95: 61
Label distribution in training epoch: Counter({1: 50, 0: 43})

Validation loss did not improve. Patience: 89/120

Epoch 96/150
Train Loss: 0.0139 | Train Acc: 100.00%
Val Loss: 2.4801 | Val Acc: 40.62%
Precision: 0.4059 | Recall: 0.4062 | F1 Score: 0.4057
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 96: 58
Label distribution in training epoch: Counter({0: 52, 1: 41})

Validation loss did not improve. Patience: 90/120

Epoch 97/150
Train Loss: 0.0021 | Train Acc: 100.00%
Val Loss: 3.3075 | Val Acc: 40.62%
Precision: 0.4591 | Recall: 0.4610 | F1 Score: 0.4057
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 97: 60
Label distribution in training epoch: Counter({0: 50, 1: 43})

Validation loss did not improve. Patience: 91/120

Epoch 98/150
Train Loss: 0.0015 | Train Acc: 100.00%
Val Loss: 2.8146 | Val Acc: 56.25%
Precision: 0.5709 | Recall: 0.5686 | F1 Score: 0.5608
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 98: 60
Label distribution in training epoch: Counter({1: 47, 0: 46})

Validation loss did not improve. Patience: 92/120

Epoch 99/150
Train Loss: 0.0075 | Train Acc: 100.00%
Val Loss: 2.7817 | Val Acc: 53.12%
Precision: 0.5312 | Recall: 0.5346 | F1 Score: 0.5195
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 99: 60
Label distribution in training epoch: Counter({1: 52, 0: 41})

Validation loss did not improve. Patience: 93/120

Epoch 100/150
Train Loss: 0.0079 | Train Acc: 100.00%
Val Loss: 3.2409 | Val Acc: 50.00%
Precision: 0.5182 | Recall: 0.5182 | F1 Score: 0.5000
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 100: 59
Label distribution in training epoch: Counter({0: 51, 1: 42})

Validation loss did not improve. Patience: 94/120

Epoch 101/150
Train Loss: 0.0107 | Train Acc: 100.00%
Val Loss: 2.5933 | Val Acc: 59.38%
Precision: 0.5913 | Recall: 0.5902 | F1 Score: 0.5901
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 101: 54
Label distribution in training epoch: Counter({0: 47, 1: 46})

Validation loss did not improve. Patience: 95/120

Epoch 102/150
Train Loss: 0.0102 | Train Acc: 98.92%
Val Loss: 2.3261 | Val Acc: 46.88%
Precision: 0.4952 | Recall: 0.4960 | F1 Score: 0.4555
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 102: 64
Label distribution in training epoch: Counter({0: 47, 1: 46})

Validation loss did not improve. Patience: 96/120

Epoch 103/150
Train Loss: 0.0024 | Train Acc: 100.00%
Val Loss: 2.8845 | Val Acc: 59.38%
Precision: 0.5821 | Recall: 0.5675 | F1 Score: 0.5589
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 103: 58
Label distribution in training epoch: Counter({0: 49, 1: 44})

Validation loss did not improve. Patience: 97/120

Epoch 104/150
Train Loss: 0.0060 | Train Acc: 100.00%
Val Loss: 2.1382 | Val Acc: 50.00%
Precision: 0.5000 | Recall: 0.5000 | F1 Score: 0.4980
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 104: 61
Label distribution in training epoch: Counter({0: 48, 1: 45})

Validation loss did not improve. Patience: 98/120

Epoch 105/150
Train Loss: 0.0140 | Train Acc: 98.92%
Val Loss: 3.2699 | Val Acc: 46.88%
Precision: 0.5040 | Recall: 0.5045 | F1 Score: 0.4640
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 105: 55
Label distribution in training epoch: Counter({0: 53, 1: 40})

Validation loss did not improve. Patience: 99/120

Epoch 106/150
Train Loss: 0.0013 | Train Acc: 100.00%
Val Loss: 3.0314 | Val Acc: 37.50%
Precision: 0.3725 | Recall: 0.3725 | F1 Score: 0.3725
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 106: 61
Label distribution in training epoch: Counter({1: 54, 0: 39})

Validation loss did not improve. Patience: 100/120

Epoch 107/150
Train Loss: 0.0173 | Train Acc: 98.92%
Val Loss: 2.0980 | Val Acc: 50.00%
Precision: 0.5000 | Recall: 0.5000 | F1 Score: 0.4921
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 107: 58
Label distribution in training epoch: Counter({1: 54, 0: 39})

Validation loss did not improve. Patience: 101/120

Epoch 108/150
Train Loss: 0.4056 | Train Acc: 92.47%
Val Loss: 3.2061 | Val Acc: 53.12%
Precision: 0.5250 | Recall: 0.5235 | F1 Score: 0.5195
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 108: 52
Label distribution in training epoch: Counter({0: 47, 1: 46})

Validation loss did not improve. Patience: 102/120

Epoch 109/150
Train Loss: 0.0221 | Train Acc: 98.92%
Val Loss: 2.9396 | Val Acc: 46.88%
Precision: 0.4870 | Recall: 0.4881 | F1 Score: 0.4640
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 109: 56
Label distribution in training epoch: Counter({1: 59, 0: 34})

Validation loss did not improve. Patience: 103/120

Epoch 110/150
Train Loss: 0.0387 | Train Acc: 97.85%
Val Loss: 1.7887 | Val Acc: 53.12%
Precision: 0.5083 | Recall: 0.5081 | F1 Score: 0.5077
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 110: 54
Label distribution in training epoch: Counter({0: 56, 1: 37})

Validation loss did not improve. Patience: 104/120

Epoch 111/150
Train Loss: 0.0022 | Train Acc: 100.00%
Val Loss: 2.3065 | Val Acc: 50.00%
Precision: 0.5000 | Recall: 0.5000 | F1 Score: 0.4921
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 111: 62
Label distribution in training epoch: Counter({1: 55, 0: 38})

Validation loss did not improve. Patience: 105/120

Epoch 112/150
Train Loss: 0.0051 | Train Acc: 100.00%
Val Loss: 1.9912 | Val Acc: 46.88%
Precision: 0.4686 | Recall: 0.4688 | F1 Score: 0.4682
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 112: 57
Label distribution in training epoch: Counter({0: 47, 1: 46})

Validation loss did not improve. Patience: 106/120

Epoch 113/150
Train Loss: 0.0051 | Train Acc: 100.00%
Val Loss: 1.9497 | Val Acc: 53.12%
Precision: 0.5278 | Recall: 0.5275 | F1 Score: 0.5271
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 113: 57
Label distribution in training epoch: Counter({0: 50, 1: 43})

Validation loss did not improve. Patience: 107/120

Epoch 114/150
Train Loss: 0.0004 | Train Acc: 100.00%
Val Loss: 3.6782 | Val Acc: 43.75%
Precision: 0.3314 | Recall: 0.3806 | F1 Score: 0.3455
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 114: 55
Label distribution in training epoch: Counter({1: 48, 0: 45})

Validation loss did not improve. Patience: 108/120

Epoch 115/150
Train Loss: 0.0014 | Train Acc: 100.00%
Val Loss: 1.7376 | Val Acc: 71.88%
Precision: 0.7146 | Recall: 0.7103 | F1 Score: 0.7117
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 115: 55
Label distribution in training epoch: Counter({0: 58, 1: 35})

Validation loss did not improve. Patience: 109/120

Epoch 116/150
Train Loss: 0.0108 | Train Acc: 100.00%
Val Loss: 2.4880 | Val Acc: 46.88%
Precision: 0.4643 | Recall: 0.4647 | F1 Score: 0.4640
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 116: 55
Label distribution in training epoch: Counter({0: 57, 1: 36})

Validation loss did not improve. Patience: 110/120

Epoch 117/150
Train Loss: 0.0060 | Train Acc: 100.00%
Val Loss: 1.9866 | Val Acc: 71.88%
Precision: 0.7136 | Recall: 0.6903 | F1 Score: 0.6946
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 117: 58
Label distribution in training epoch: Counter({1: 56, 0: 37})

Validation loss did not improve. Patience: 111/120

Epoch 118/150
Train Loss: 0.0014 | Train Acc: 100.00%
Val Loss: 2.8164 | Val Acc: 56.25%
Precision: 0.5833 | Recall: 0.5794 | F1 Score: 0.5608
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 118: 55
Label distribution in training epoch: Counter({1: 49, 0: 44})

Validation loss did not improve. Patience: 112/120

Epoch 119/150
Train Loss: 0.0264 | Train Acc: 98.92%
Val Loss: 2.6659 | Val Acc: 59.38%
Precision: 0.6039 | Recall: 0.5938 | F1 Score: 0.5836
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 119: 56
Label distribution in training epoch: Counter({0: 53, 1: 40})

Validation loss did not improve. Patience: 113/120

Epoch 120/150
Train Loss: 0.0012 | Train Acc: 100.00%
Val Loss: 2.2560 | Val Acc: 65.62%
Precision: 0.6627 | Recall: 0.6608 | F1 Score: 0.6559
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 120: 60
Label distribution in training epoch: Counter({0: 49, 1: 44})

Validation loss did not improve. Patience: 114/120

Epoch 121/150
Train Loss: 0.0016 | Train Acc: 100.00%
Val Loss: 2.2607 | Val Acc: 65.62%
Precision: 0.6948 | Recall: 0.6786 | F1 Score: 0.6532
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 121: 61
Label distribution in training epoch: Counter({0: 48, 1: 45})

Validation loss did not improve. Patience: 115/120

Epoch 122/150
Train Loss: 0.0020 | Train Acc: 100.00%
Val Loss: 3.6527 | Val Acc: 37.50%
Precision: 0.3667 | Recall: 0.3750 | F1 Score: 0.3651
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 122: 59
Label distribution in training epoch: Counter({1: 53, 0: 40})

Validation loss did not improve. Patience: 116/120

Epoch 123/150
Train Loss: 0.0003 | Train Acc: 100.00%
Val Loss: 3.3435 | Val Acc: 53.12%
Precision: 0.5583 | Recall: 0.5567 | F1 Score: 0.5308
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 123: 61
Label distribution in training epoch: Counter({0: 47, 1: 46})

Validation loss did not improve. Patience: 117/120

Epoch 124/150
Train Loss: 0.0622 | Train Acc: 98.92%
Val Loss: 3.0004 | Val Acc: 40.62%
Precision: 0.4246 | Recall: 0.4177 | F1 Score: 0.4010
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 124: 63
Label distribution in training epoch: Counter({1: 52, 0: 41})

Validation loss did not improve. Patience: 118/120

Epoch 125/150
Train Loss: 0.0008 | Train Acc: 100.00%
Val Loss: 2.6529 | Val Acc: 53.12%
Precision: 0.5278 | Recall: 0.5275 | F1 Score: 0.5271
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 125: 57
Label distribution in training epoch: Counter({0: 52, 1: 41})

Validation loss did not improve. Patience: 119/120

Epoch 126/150
Train Loss: 0.0061 | Train Acc: 100.00%
Val Loss: 3.2472 | Val Acc: 43.75%
Precision: 0.4444 | Recall: 0.4444 | F1 Score: 0.4375
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 126: 54
Label distribution in training epoch: Counter({0: 53, 1: 40})

Validation loss did not improve. Patience: 120/120

Early stopping triggered after 126 epochs.


Training complete.
Loading best model from /home/etudiant/Projets/Viviane/LIDC-ML/models/best_model_resnet_pytorch3D_architecture_v0.pth for final metrics.
######## Training Finished in 1h 30m 17s ###########
Test Accuracy on 32 images: 84.38%
AUC: 0.9246
AUC: 0.8516
Class 0-non-cancer: Precision: 0.62, Recall: 0.71, F1-Score: 0.67
Class 1-cancer: Precision: 0.75, Recall: 0.67, F1-Score: 0.71
