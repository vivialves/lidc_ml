

==== Training started at 2025-07-16 20:31:44.083614 ====

Using Gradient Accumulation with 4 steps.
DataLoader batch size: 8
Effective batch size: 32

Epoch 1/100
Train Loss: 0.6966 | Train Acc: 50.54%
Val Loss: 0.7005 | Val Acc: 48.12%
Precision: 0.7357 | Recall: 0.5174 | F1 Score: 0.3541
Current AMP scale: 65536.0
Unique augmented volumes seen in epoch 1: 93
Label distribution in training epoch: Counter({1: 48, 0: 45})

Validation loss improved. Saving best model to /home/etudiant/Projets/Viviane/LIDC-ML/models/best_model_resnet_pytorch3D_architecture_v0.pth

Epoch 2/100
Train Loss: 0.6979 | Train Acc: 47.31%
Val Loss: 0.6844 | Val Acc: 57.50%
Precision: 0.6703 | Recall: 0.5545 | F1 Score: 0.4744
Current AMP scale: 65536.0
Unique augmented volumes seen in epoch 2: 93
Label distribution in training epoch: Counter({0: 47, 1: 46})

Validation loss improved. Saving best model to /home/etudiant/Projets/Viviane/LIDC-ML/models/best_model_resnet_pytorch3D_architecture_v0.pth

Epoch 3/100
Train Loss: 0.6747 | Train Acc: 62.37%
Val Loss: 0.7066 | Val Acc: 51.25%
Precision: 0.6016 | Recall: 0.5671 | F1 Score: 0.4894
Current AMP scale: 65536.0
Unique augmented volumes seen in epoch 3: 93
Label distribution in training epoch: Counter({0: 53, 1: 40})

Validation loss did not improve. Patience: 1/50

Epoch 4/100
Train Loss: 0.6841 | Train Acc: 56.99%
Val Loss: 0.6633 | Val Acc: 64.38%
Precision: 0.6458 | Recall: 0.6115 | F1 Score: 0.6035
Current AMP scale: 65536.0
Unique augmented volumes seen in epoch 4: 93
Label distribution in training epoch: Counter({1: 52, 0: 41})

Validation loss improved. Saving best model to /home/etudiant/Projets/Viviane/LIDC-ML/models/best_model_resnet_pytorch3D_architecture_v0.pth

Epoch 5/100
Train Loss: 0.7202 | Train Acc: 53.76%
Val Loss: 0.7077 | Val Acc: 48.12%
Precision: 0.4781 | Recall: 0.4789 | F1 Score: 0.4753
Current AMP scale: 65536.0
Unique augmented volumes seen in epoch 5: 92
Label distribution in training epoch: Counter({1: 51, 0: 42})

Validation loss did not improve. Patience: 1/50

Epoch 6/100
Train Loss: 0.5818 | Train Acc: 74.19%
Val Loss: 0.7195 | Val Acc: 53.75%
Precision: 0.5359 | Recall: 0.5361 | F1 Score: 0.5357
Current AMP scale: 65536.0
Unique augmented volumes seen in epoch 6: 93
Label distribution in training epoch: Counter({0: 51, 1: 42})

Validation loss did not improve. Patience: 2/50

Epoch 7/100
Train Loss: 0.6115 | Train Acc: 68.82%
Val Loss: 0.7323 | Val Acc: 57.50%
Precision: 0.6034 | Recall: 0.5978 | F1 Score: 0.5733
Current AMP scale: 65536.0
Unique augmented volumes seen in epoch 7: 93
Label distribution in training epoch: Counter({0: 49, 1: 44})

Validation loss did not improve. Patience: 3/50

Epoch 8/100
Train Loss: 0.6341 | Train Acc: 62.37%
Val Loss: 0.7201 | Val Acc: 64.38%
Precision: 0.6101 | Recall: 0.5950 | F1 Score: 0.5960
Current AMP scale: 65536.0
Unique augmented volumes seen in epoch 8: 92
Label distribution in training epoch: Counter({1: 54, 0: 39})

Validation loss did not improve. Patience: 4/50

Epoch 9/100
Train Loss: 0.5843 | Train Acc: 65.59%
Val Loss: 0.9185 | Val Acc: 50.62%
Precision: 0.4926 | Recall: 0.4929 | F1 Score: 0.4918
Current AMP scale: 65536.0
Unique augmented volumes seen in epoch 9: 93
Label distribution in training epoch: Counter({1: 49, 0: 44})

Validation loss did not improve. Patience: 5/50

Epoch 10/100
Train Loss: 0.5355 | Train Acc: 74.19%
Val Loss: 0.9099 | Val Acc: 48.75%
Precision: 0.4667 | Recall: 0.4695 | F1 Score: 0.4633
Current AMP scale: 65536.0
Unique augmented volumes seen in epoch 10: 93
Label distribution in training epoch: Counter({0: 53, 1: 40})

Validation loss did not improve. Patience: 6/50

Epoch 11/100
Train Loss: 0.6057 | Train Acc: 68.82%
Val Loss: 0.8150 | Val Acc: 51.25%
Precision: 0.5035 | Recall: 0.5034 | F1 Score: 0.5031
Current AMP scale: 65536.0
Unique augmented volumes seen in epoch 11: 93
Label distribution in training epoch: Counter({1: 48, 0: 45})

Validation loss did not improve. Patience: 7/50

Epoch 12/100
Train Loss: 0.5852 | Train Acc: 66.67%
Val Loss: 0.7597 | Val Acc: 44.38%
Precision: 0.4441 | Recall: 0.4439 | F1 Score: 0.4436
Current AMP scale: 65536.0
Unique augmented volumes seen in epoch 12: 93
Label distribution in training epoch: Counter({0: 50, 1: 43})

Validation loss did not improve. Patience: 8/50

Epoch 13/100
Train Loss: 0.5152 | Train Acc: 78.49%
Val Loss: 0.7819 | Val Acc: 45.00%
Precision: 0.4492 | Recall: 0.4493 | F1 Score: 0.4492
Current AMP scale: 65536.0
Unique augmented volumes seen in epoch 13: 93
Label distribution in training epoch: Counter({0: 47, 1: 46})

Validation loss did not improve. Patience: 9/50

Epoch 14/100
Train Loss: 0.5407 | Train Acc: 72.04%
Val Loss: 0.8538 | Val Acc: 44.38%
Precision: 0.4533 | Recall: 0.4587 | F1 Score: 0.4340
Current AMP scale: 65536.0
Unique augmented volumes seen in epoch 14: 93
Label distribution in training epoch: Counter({0: 47, 1: 46})

Validation loss did not improve. Patience: 10/50

Epoch 15/100
Train Loss: 0.5512 | Train Acc: 72.04%
Val Loss: 0.7750 | Val Acc: 61.25%
Precision: 0.6243 | Recall: 0.6021 | F1 Score: 0.5893
Current AMP scale: 65536.0
Unique augmented volumes seen in epoch 15: 93
Label distribution in training epoch: Counter({0: 49, 1: 44})

Validation loss did not improve. Patience: 11/50

Epoch 16/100
Train Loss: 0.4513 | Train Acc: 81.72%
Val Loss: 0.9538 | Val Acc: 55.62%
Precision: 0.5738 | Recall: 0.5562 | F1 Score: 0.5282
Current AMP scale: 65536.0
Unique augmented volumes seen in epoch 16: 93
Label distribution in training epoch: Counter({0: 49, 1: 44})

Validation loss did not improve. Patience: 12/50

Epoch 17/100
Train Loss: 0.4749 | Train Acc: 74.19%
Val Loss: 0.9127 | Val Acc: 49.38%
Precision: 0.5009 | Recall: 0.5008 | F1 Score: 0.4789
Current AMP scale: 65536.0
Unique augmented volumes seen in epoch 17: 93
Label distribution in training epoch: Counter({1: 48, 0: 45})

Validation loss did not improve. Patience: 13/50

Epoch 18/100
Train Loss: 0.5070 | Train Acc: 75.27%
Val Loss: 0.9278 | Val Acc: 53.75%
Precision: 0.5339 | Recall: 0.5341 | F1 Score: 0.5339
Current AMP scale: 65536.0
Unique augmented volumes seen in epoch 18: 92
Label distribution in training epoch: Counter({1: 49, 0: 44})

Validation loss did not improve. Patience: 14/50

Epoch 19/100
Train Loss: 0.5270 | Train Acc: 76.34%
Val Loss: 1.0329 | Val Acc: 48.12%
Precision: 0.5074 | Recall: 0.5067 | F1 Score: 0.4767
Current AMP scale: 65536.0
Unique augmented volumes seen in epoch 19: 93
Label distribution in training epoch: Counter({1: 47, 0: 46})

Validation loss did not improve. Patience: 15/50

Epoch 20/100
Train Loss: 0.5021 | Train Acc: 75.27%
Val Loss: 1.0777 | Val Acc: 56.25%
Precision: 0.5668 | Recall: 0.5638 | F1 Score: 0.5581
Current AMP scale: 65536.0
Unique augmented volumes seen in epoch 20: 93
Label distribution in training epoch: Counter({1: 53, 0: 40})

Validation loss did not improve. Patience: 16/50

Epoch 21/100
Train Loss: 0.5062 | Train Acc: 74.19%
Val Loss: 1.1018 | Val Acc: 53.12%
Precision: 0.5308 | Recall: 0.5308 | F1 Score: 0.5308
Current AMP scale: 65536.0
Unique augmented volumes seen in epoch 21: 93
Label distribution in training epoch: Counter({0: 56, 1: 37})

Validation loss did not improve. Patience: 17/50

Epoch 22/100
Train Loss: 0.4309 | Train Acc: 78.49%
Val Loss: 1.0786 | Val Acc: 49.38%
Precision: 0.4894 | Recall: 0.4895 | F1 Score: 0.4893
Current AMP scale: 65536.0
Unique augmented volumes seen in epoch 22: 92
Label distribution in training epoch: Counter({0: 50, 1: 43})

Validation loss did not improve. Patience: 18/50

Epoch 23/100
Train Loss: 0.3809 | Train Acc: 82.80%
Val Loss: 0.8617 | Val Acc: 52.50%
Precision: 0.5150 | Recall: 0.5146 | F1 Score: 0.5141
Current AMP scale: 65536.0
Unique augmented volumes seen in epoch 23: 93
Label distribution in training epoch: Counter({0: 55, 1: 38})

Validation loss did not improve. Patience: 19/50

Epoch 24/100
Train Loss: 0.3945 | Train Acc: 81.72%
Val Loss: 0.8731 | Val Acc: 51.25%
Precision: 0.5070 | Recall: 0.5062 | F1 Score: 0.4947
Current AMP scale: 65536.0
Unique augmented volumes seen in epoch 24: 93
Label distribution in training epoch: Counter({0: 49, 1: 44})

Validation loss did not improve. Patience: 20/50

Epoch 25/100
Train Loss: 0.4868 | Train Acc: 80.65%
Val Loss: 0.9129 | Val Acc: 47.50%
Precision: 0.4504 | Recall: 0.4555 | F1 Score: 0.4470
Current AMP scale: 65536.0
Unique augmented volumes seen in epoch 25: 93
Label distribution in training epoch: Counter({1: 53, 0: 40})

Validation loss did not improve. Patience: 21/50

Epoch 26/100
Train Loss: 0.3981 | Train Acc: 81.72%
Val Loss: 1.0394 | Val Acc: 54.38%
Precision: 0.5440 | Recall: 0.5427 | F1 Score: 0.5397
Current AMP scale: 65536.0
Unique augmented volumes seen in epoch 26: 93
Label distribution in training epoch: Counter({1: 47, 0: 46})

Validation loss did not improve. Patience: 22/50

Epoch 27/100
Train Loss: 0.4412 | Train Acc: 74.19%
Val Loss: 1.2146 | Val Acc: 51.25%
Precision: 0.5145 | Recall: 0.5138 | F1 Score: 0.5076
Current AMP scale: 65536.0
Unique augmented volumes seen in epoch 27: 93
Label distribution in training epoch: Counter({1: 50, 0: 43})

Validation loss did not improve. Patience: 23/50

Epoch 28/100
Train Loss: 0.3514 | Train Acc: 87.10%
Val Loss: 1.2118 | Val Acc: 55.00%
Precision: 0.5545 | Recall: 0.5531 | F1 Score: 0.5482
Current AMP scale: 65536.0
Unique augmented volumes seen in epoch 28: 93
Label distribution in training epoch: Counter({1: 52, 0: 41})

Validation loss did not improve. Patience: 24/50

Epoch 29/100
Train Loss: 0.3907 | Train Acc: 84.95%
Val Loss: 1.6476 | Val Acc: 45.00%
Precision: 0.4570 | Recall: 0.4723 | F1 Score: 0.4093
Current AMP scale: 65536.0
Unique augmented volumes seen in epoch 29: 93
Label distribution in training epoch: Counter({0: 55, 1: 38})

Validation loss did not improve. Patience: 25/50

Epoch 30/100
Train Loss: 0.3873 | Train Acc: 82.80%
Val Loss: 1.3856 | Val Acc: 48.75%
Precision: 0.5000 | Recall: 0.5000 | F1 Score: 0.4868
Current AMP scale: 65536.0
Unique augmented volumes seen in epoch 30: 93
Label distribution in training epoch: Counter({0: 48, 1: 45})

Validation loss did not improve. Patience: 26/50

Epoch 31/100
Train Loss: 0.2876 | Train Acc: 88.17%
Val Loss: 1.3276 | Val Acc: 48.12%
Precision: 0.4809 | Recall: 0.4812 | F1 Score: 0.4788
Current AMP scale: 65536.0
Unique augmented volumes seen in epoch 31: 93
Label distribution in training epoch: Counter({1: 48, 0: 45})

Validation loss did not improve. Patience: 27/50

Epoch 32/100
Train Loss: 0.2735 | Train Acc: 88.17%
Val Loss: 1.3969 | Val Acc: 50.62%
Precision: 0.5155 | Recall: 0.5133 | F1 Score: 0.4918
Current AMP scale: 65536.0
Unique augmented volumes seen in epoch 32: 93
Label distribution in training epoch: Counter({1: 49, 0: 44})

Validation loss did not improve. Patience: 28/50

Epoch 33/100
Train Loss: 0.2416 | Train Acc: 91.40%
Val Loss: 1.0858 | Val Acc: 53.75%
Precision: 0.5291 | Recall: 0.5287 | F1 Score: 0.5286
Current AMP scale: 65536.0
Unique augmented volumes seen in epoch 33: 93
Label distribution in training epoch: Counter({1: 48, 0: 45})

Validation loss did not improve. Patience: 29/50

Epoch 34/100
Train Loss: 0.2590 | Train Acc: 89.25%
Val Loss: 1.2922 | Val Acc: 48.12%
Precision: 0.4812 | Recall: 0.4812 | F1 Score: 0.4812
Current AMP scale: 65536.0
Unique augmented volumes seen in epoch 34: 93
Label distribution in training epoch: Counter({1: 48, 0: 45})

Validation loss did not improve. Patience: 30/50

Epoch 35/100
Train Loss: 0.3602 | Train Acc: 80.65%
Val Loss: 1.4940 | Val Acc: 45.62%
Precision: 0.4486 | Recall: 0.4508 | F1 Score: 0.4467
Current AMP scale: 65536.0
Unique augmented volumes seen in epoch 35: 93
Label distribution in training epoch: Counter({1: 48, 0: 45})

Validation loss did not improve. Patience: 31/50

Epoch 36/100
Train Loss: 0.2404 | Train Acc: 90.32%
Val Loss: 1.3486 | Val Acc: 45.62%
Precision: 0.4476 | Recall: 0.4467 | F1 Score: 0.4467
Current AMP scale: 65536.0
Unique augmented volumes seen in epoch 36: 93
Label distribution in training epoch: Counter({1: 52, 0: 41})

Validation loss did not improve. Patience: 32/50

Epoch 37/100
Train Loss: 0.2331 | Train Acc: 88.17%
Val Loss: 1.5173 | Val Acc: 51.25%
Precision: 0.5220 | Recall: 0.5201 | F1 Score: 0.5048
Current AMP scale: 65536.0
Unique augmented volumes seen in epoch 37: 93
Label distribution in training epoch: Counter({1: 47, 0: 46})

Validation loss did not improve. Patience: 33/50

Epoch 38/100
Train Loss: 0.2677 | Train Acc: 90.32%
Val Loss: 1.7149 | Val Acc: 48.12%
Precision: 0.4770 | Recall: 0.4783 | F1 Score: 0.4722
Current AMP scale: 65536.0
Unique augmented volumes seen in epoch 38: 93
Label distribution in training epoch: Counter({0: 50, 1: 43})

Validation loss did not improve. Patience: 34/50

Epoch 39/100
Train Loss: 0.2713 | Train Acc: 88.17%
Val Loss: 1.4578 | Val Acc: 44.38%
Precision: 0.4402 | Recall: 0.4408 | F1 Score: 0.4401
Current AMP scale: 65536.0
Unique augmented volumes seen in epoch 39: 93
Label distribution in training epoch: Counter({1: 51, 0: 42})

Validation loss did not improve. Patience: 35/50

Epoch 40/100
Train Loss: 0.2297 | Train Acc: 90.32%
Val Loss: 1.6425 | Val Acc: 37.50%
Precision: 0.3757 | Recall: 0.3765 | F1 Score: 0.3746
Current AMP scale: 65536.0
Unique augmented volumes seen in epoch 40: 93
Label distribution in training epoch: Counter({0: 48, 1: 45})

Validation loss did not improve. Patience: 36/50

Epoch 41/100
Train Loss: 0.2033 | Train Acc: 92.47%
Val Loss: 1.6075 | Val Acc: 40.62%
Precision: 0.4077 | Recall: 0.4107 | F1 Score: 0.4034
Current AMP scale: 65536.0
Unique augmented volumes seen in epoch 41: 93
Label distribution in training epoch: Counter({0: 56, 1: 37})

Validation loss did not improve. Patience: 37/50

Epoch 42/100
Train Loss: 0.1693 | Train Acc: 96.77%
Val Loss: 1.7689 | Val Acc: 46.88%
Precision: 0.4677 | Recall: 0.4706 | F1 Score: 0.4575
Current AMP scale: 65536.0
Unique augmented volumes seen in epoch 42: 93
Label distribution in training epoch: Counter({0: 56, 1: 37})

Validation loss did not improve. Patience: 38/50

Epoch 43/100
Train Loss: 0.2211 | Train Acc: 91.40%
Val Loss: 1.8015 | Val Acc: 48.12%
Precision: 0.4890 | Recall: 0.4898 | F1 Score: 0.4753
Current AMP scale: 65536.0
Unique augmented volumes seen in epoch 43: 93
Label distribution in training epoch: Counter({1: 52, 0: 41})

Validation loss did not improve. Patience: 39/50

Epoch 44/100
Train Loss: 0.1624 | Train Acc: 92.47%
Val Loss: 1.8292 | Val Acc: 54.38%
Precision: 0.5482 | Recall: 0.5414 | F1 Score: 0.5260
Current AMP scale: 65536.0
Unique augmented volumes seen in epoch 44: 93
Label distribution in training epoch: Counter({0: 53, 1: 40})

Validation loss did not improve. Patience: 40/50

Epoch 45/100
Train Loss: 0.2700 | Train Acc: 91.40%
Val Loss: 1.6187 | Val Acc: 48.12%
Precision: 0.4854 | Recall: 0.4859 | F1 Score: 0.4788
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 45: 93
Label distribution in training epoch: Counter({1: 53, 0: 40})

Validation loss did not improve. Patience: 41/50

Epoch 46/100
Train Loss: 0.2016 | Train Acc: 93.55%
Val Loss: 1.9139 | Val Acc: 48.75%
Precision: 0.4883 | Recall: 0.4896 | F1 Score: 0.4736
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 46: 93
Label distribution in training epoch: Counter({1: 52, 0: 41})

Validation loss did not improve. Patience: 42/50

Epoch 47/100
Train Loss: 0.1800 | Train Acc: 93.55%
Val Loss: 1.2664 | Val Acc: 56.25%
Precision: 0.5635 | Recall: 0.5625 | F1 Score: 0.5608
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 47: 93
Label distribution in training epoch: Counter({0: 48, 1: 45})

Validation loss did not improve. Patience: 43/50

Epoch 48/100
Train Loss: 0.1686 | Train Acc: 92.47%
Val Loss: 1.6698 | Val Acc: 46.88%
Precision: 0.4640 | Recall: 0.4688 | F1 Score: 0.4507
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 48: 93
Label distribution in training epoch: Counter({0: 51, 1: 42})

Validation loss did not improve. Patience: 44/50

Epoch 49/100
Train Loss: 0.2839 | Train Acc: 90.32%
Val Loss: 1.6382 | Val Acc: 51.25%
Precision: 0.5238 | Recall: 0.5184 | F1 Score: 0.4865
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 49: 93
Label distribution in training epoch: Counter({1: 50, 0: 43})

Validation loss did not improve. Patience: 45/50

Epoch 50/100
Train Loss: 0.2631 | Train Acc: 89.25%
Val Loss: 1.6597 | Val Acc: 43.12%
Precision: 0.4294 | Recall: 0.4305 | F1 Score: 0.4285
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 50: 93
Label distribution in training epoch: Counter({0: 58, 1: 35})

Validation loss did not improve. Patience: 46/50

Epoch 51/100
Train Loss: 0.1850 | Train Acc: 95.70%
Val Loss: 1.6259 | Val Acc: 43.75%
Precision: 0.4167 | Recall: 0.4206 | F1 Score: 0.4170
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 51: 92
Label distribution in training epoch: Counter({0: 48, 1: 45})

Validation loss did not improve. Patience: 47/50

Epoch 52/100
Train Loss: 0.1178 | Train Acc: 96.77%
Val Loss: 1.6918 | Val Acc: 44.38%
Precision: 0.4339 | Recall: 0.4341 | F1 Score: 0.4340
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 52: 93
Label distribution in training epoch: Counter({0: 47, 1: 46})

Validation loss did not improve. Patience: 48/50

Epoch 53/100
Train Loss: 0.1457 | Train Acc: 94.62%
Val Loss: 1.6377 | Val Acc: 46.88%
Precision: 0.4689 | Recall: 0.4689 | F1 Score: 0.4687
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 53: 93
Label distribution in training epoch: Counter({1: 52, 0: 41})

Validation loss did not improve. Patience: 49/50

Epoch 54/100
Train Loss: 0.2341 | Train Acc: 91.40%
Val Loss: 1.5978 | Val Acc: 50.62%
Precision: 0.4962 | Recall: 0.4964 | F1 Score: 0.4939
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 54: 93
Label distribution in training epoch: Counter({1: 50, 0: 43})

Validation loss did not improve. Patience: 50/50

Early stopping triggered after 54 epochs.


Training complete.
Loading best model from /home/etudiant/Projets/Viviane/LIDC-ML/models/best_model_resnet_pytorch3D_architecture_v0.pth for final metrics.
######## Training Finished in 17h 51m 4s ###########
Test Accuracy on 160 images: 72.50%
AUC: 0.5700
Class 0-non-cancer: Precision: 0.59, Recall: 0.78, F1-Score: 0.67
Class 1-cancer: Precision: 0.60, Recall: 0.38, F1-Score: 0.46
