

==== Training started at 2025-07-15 17:02:33.752025 ====

Using Gradient Accumulation with 4 steps.
DataLoader batch size: 4
Effective batch size: 16

Epoch 1/100
Train Loss: 0.7182 | Train Acc: 54.84%
Val Loss: 0.6894 | Val Acc: 56.25%
Precision: 0.2812 | Recall: 0.5000 | F1 Score: 0.3600
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 1: 54
Label distribution in training epoch: Counter({1: 48, 0: 45})

Validation loss improved. Saving best model to /home/etudiant/Projets/Viviane/LIDC-ML/models/best_model_resnet_pytorch3D_architecture_v0.pth

Epoch 2/100
Train Loss: 0.6640 | Train Acc: 60.22%
Val Loss: 0.6612 | Val Acc: 62.50%
Precision: 0.6494 | Recall: 0.6353 | F1 Score: 0.6190
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 2: 62
Label distribution in training epoch: Counter({1: 51, 0: 42})

Validation loss improved. Saving best model to /home/etudiant/Projets/Viviane/LIDC-ML/models/best_model_resnet_pytorch3D_architecture_v0.pth

Epoch 3/100
Train Loss: 0.6181 | Train Acc: 65.59%
Val Loss: 0.6765 | Val Acc: 65.62%
Precision: 0.6594 | Recall: 0.6310 | F1 Score: 0.6267
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 3: 60
Label distribution in training epoch: Counter({0: 47, 1: 46})

Validation loss did not improve. Patience: 1/50

Epoch 4/100
Train Loss: 0.6526 | Train Acc: 54.84%
Val Loss: 0.6460 | Val Acc: 65.62%
Precision: 0.7286 | Recall: 0.6562 | F1 Score: 0.6267
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 4: 67
Label distribution in training epoch: Counter({0: 49, 1: 44})

Validation loss improved. Saving best model to /home/etudiant/Projets/Viviane/LIDC-ML/models/best_model_resnet_pytorch3D_architecture_v0.pth

Epoch 5/100
Train Loss: 0.6519 | Train Acc: 64.52%
Val Loss: 0.7143 | Val Acc: 43.75%
Precision: 0.4286 | Recall: 0.4286 | F1 Score: 0.4286
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 5: 54
Label distribution in training epoch: Counter({0: 54, 1: 39})

Validation loss did not improve. Patience: 1/50

Epoch 6/100
Train Loss: 0.6231 | Train Acc: 67.74%
Val Loss: 0.7159 | Val Acc: 59.38%
Precision: 0.6458 | Recall: 0.6098 | F1 Score: 0.5733
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 6: 60
Label distribution in training epoch: Counter({1: 59, 0: 34})

Validation loss did not improve. Patience: 2/50

Epoch 7/100
Train Loss: 0.6092 | Train Acc: 65.59%
Val Loss: 0.7164 | Val Acc: 56.25%
Precision: 0.5466 | Recall: 0.5466 | F1 Score: 0.5466
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 7: 59
Label distribution in training epoch: Counter({1: 50, 0: 43})

Validation loss did not improve. Patience: 3/50

Epoch 8/100
Train Loss: 0.4470 | Train Acc: 84.95%
Val Loss: 0.8212 | Val Acc: 56.25%
Precision: 0.5830 | Recall: 0.5830 | F1 Score: 0.5625
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 8: 60
Label distribution in training epoch: Counter({0: 51, 1: 42})

Validation loss did not improve. Patience: 4/50

Epoch 9/100
Train Loss: 0.4705 | Train Acc: 76.34%
Val Loss: 1.0562 | Val Acc: 43.75%
Precision: 0.4365 | Recall: 0.4375 | F1 Score: 0.4353
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 9: 62
Label distribution in training epoch: Counter({1: 54, 0: 39})

Validation loss did not improve. Patience: 5/50

Epoch 10/100
Train Loss: 0.4891 | Train Acc: 79.57%
Val Loss: 1.2509 | Val Acc: 40.62%
Precision: 0.4500 | Recall: 0.4879 | F1 Score: 0.3267
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 10: 58
Label distribution in training epoch: Counter({0: 51, 1: 42})

Validation loss did not improve. Patience: 6/50

Epoch 11/100
Train Loss: 0.3676 | Train Acc: 89.25%
Val Loss: 1.0128 | Val Acc: 50.00%
Precision: 0.4892 | Recall: 0.4902 | F1 Score: 0.4818
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 11: 57
Label distribution in training epoch: Counter({0: 50, 1: 43})

Validation loss did not improve. Patience: 7/50

Epoch 12/100
Train Loss: 0.3663 | Train Acc: 81.72%
Val Loss: 1.0552 | Val Acc: 43.75%
Precision: 0.3407 | Recall: 0.4157 | F1 Score: 0.3455
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 12: 55
Label distribution in training epoch: Counter({0: 53, 1: 40})

Validation loss did not improve. Patience: 8/50

Epoch 13/100
Train Loss: 0.3354 | Train Acc: 87.10%
Val Loss: 1.7332 | Val Acc: 53.12%
Precision: 0.5192 | Recall: 0.5118 | F1 Score: 0.4684
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 13: 58
Label distribution in training epoch: Counter({0: 51, 1: 42})

Validation loss did not improve. Patience: 9/50

Epoch 14/100
Train Loss: 0.4229 | Train Acc: 83.87%
Val Loss: 1.5928 | Val Acc: 25.00%
Precision: 0.2833 | Recall: 0.2636 | F1 Score: 0.2471
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 14: 59
Label distribution in training epoch: Counter({1: 51, 0: 42})

Validation loss did not improve. Patience: 10/50

Epoch 15/100
Train Loss: 0.3237 | Train Acc: 87.10%
Val Loss: 1.0517 | Val Acc: 50.00%
Precision: 0.4921 | Recall: 0.4921 | F1 Score: 0.4921
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 15: 60
Label distribution in training epoch: Counter({1: 53, 0: 40})

Validation loss did not improve. Patience: 11/50

Epoch 16/100
Train Loss: 0.2735 | Train Acc: 91.40%
Val Loss: 1.3281 | Val Acc: 34.38%
Precision: 0.3591 | Recall: 0.3745 | F1 Score: 0.3379
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 16: 50
Label distribution in training epoch: Counter({0: 61, 1: 32})

Validation loss did not improve. Patience: 12/50

Epoch 17/100
Train Loss: 0.3044 | Train Acc: 87.10%
Val Loss: 1.4713 | Val Acc: 31.25%
Precision: 0.2996 | Recall: 0.3059 | F1 Score: 0.3016
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 17: 58
Label distribution in training epoch: Counter({1: 48, 0: 45})

Validation loss did not improve. Patience: 13/50

Epoch 18/100
Train Loss: 0.1284 | Train Acc: 98.92%
Val Loss: 1.6529 | Val Acc: 53.12%
Precision: 0.5625 | Recall: 0.5471 | F1 Score: 0.5077
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 18: 58
Label distribution in training epoch: Counter({0: 54, 1: 39})

Validation loss did not improve. Patience: 14/50

Epoch 19/100
Train Loss: 0.1651 | Train Acc: 93.55%
Val Loss: 1.3185 | Val Acc: 50.00%
Precision: 0.4941 | Recall: 0.4939 | F1 Score: 0.4921
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 19: 54
Label distribution in training epoch: Counter({0: 49, 1: 44})

Validation loss did not improve. Patience: 15/50

Epoch 20/100
Train Loss: 0.1507 | Train Acc: 95.70%
Val Loss: 1.4340 | Val Acc: 34.38%
Precision: 0.3381 | Recall: 0.3438 | F1 Score: 0.3379
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 20: 54
Label distribution in training epoch: Counter({0: 56, 1: 37})

Validation loss did not improve. Patience: 16/50

Epoch 21/100
Train Loss: 0.3576 | Train Acc: 88.17%
Val Loss: 1.5762 | Val Acc: 53.12%
Precision: 0.4943 | Recall: 0.4960 | F1 Score: 0.4684
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 21: 62
Label distribution in training epoch: Counter({1: 53, 0: 40})

Validation loss did not improve. Patience: 17/50

Epoch 22/100
Train Loss: 0.1269 | Train Acc: 96.77%
Val Loss: 1.7627 | Val Acc: 40.62%
Precision: 0.3745 | Recall: 0.3849 | F1 Score: 0.3764
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 22: 53
Label distribution in training epoch: Counter({0: 51, 1: 42})

Validation loss did not improve. Patience: 18/50

Epoch 23/100
Train Loss: 0.1370 | Train Acc: 95.70%
Val Loss: 1.3355 | Val Acc: 62.50%
Precision: 0.6277 | Recall: 0.6157 | F1 Score: 0.6113
Current AMP scale: 32768.0
Unique augmented volumes seen in epoch 23: 59
Label distribution in training epoch: Counter({0: 52, 1: 41})

Validation loss did not improve. Patience: 19/50

Epoch 24/100
Train Loss: 0.2598 | Train Acc: 93.55%
Val Loss: 1.5322 | Val Acc: 46.88%
Precision: 0.4375 | Recall: 0.4529 | F1 Score: 0.4231
Current AMP scale: 16384.0
Unique augmented volumes seen in epoch 24: 61
Label distribution in training epoch: Counter({1: 47, 0: 46})

Validation loss did not improve. Patience: 20/50

Epoch 25/100
Train Loss: 0.1305 | Train Acc: 95.70%
Val Loss: 2.7201 | Val Acc: 46.88%
Precision: 0.4792 | Recall: 0.4843 | F1 Score: 0.4421
Current AMP scale: 16384.0
Unique augmented volumes seen in epoch 25: 56
Label distribution in training epoch: Counter({0: 52, 1: 41})

Validation loss did not improve. Patience: 21/50

Epoch 26/100
Train Loss: 0.3503 | Train Acc: 91.40%
Val Loss: 1.5157 | Val Acc: 34.38%
Precision: 0.3016 | Recall: 0.2773 | F1 Score: 0.2874
Current AMP scale: 16384.0
Unique augmented volumes seen in epoch 26: 60
Label distribution in training epoch: Counter({1: 52, 0: 41})

Validation loss did not improve. Patience: 22/50

Epoch 27/100
Train Loss: 0.1452 | Train Acc: 95.70%
Val Loss: 1.7450 | Val Acc: 40.62%
Precision: 0.3841 | Recall: 0.4062 | F1 Score: 0.3764
Current AMP scale: 16384.0
Unique augmented volumes seen in epoch 27: 56
Label distribution in training epoch: Counter({1: 47, 0: 46})

Validation loss did not improve. Patience: 23/50

Epoch 28/100
Train Loss: 0.2072 | Train Acc: 91.40%
Val Loss: 1.5887 | Val Acc: 59.38%
Precision: 0.5833 | Recall: 0.5850 | F1 Score: 0.5836
Current AMP scale: 16384.0
Unique augmented volumes seen in epoch 28: 61
Label distribution in training epoch: Counter({1: 49, 0: 44})

Validation loss did not improve. Patience: 24/50

Epoch 29/100
Train Loss: 0.3287 | Train Acc: 86.02%
Val Loss: 2.1165 | Val Acc: 56.25%
Precision: 0.6017 | Recall: 0.5951 | F1 Score: 0.5608
Current AMP scale: 16384.0
Unique augmented volumes seen in epoch 29: 57
Label distribution in training epoch: Counter({0: 54, 1: 39})

Validation loss did not improve. Patience: 25/50

Epoch 30/100
Train Loss: 0.2640 | Train Acc: 87.10%
Val Loss: 1.7788 | Val Acc: 56.25%
Precision: 0.6167 | Recall: 0.6273 | F1 Score: 0.5608
Current AMP scale: 16384.0
Unique augmented volumes seen in epoch 30: 56
Label distribution in training epoch: Counter({1: 52, 0: 41})

Validation loss did not improve. Patience: 26/50

Epoch 31/100
Train Loss: 0.2443 | Train Acc: 88.17%
Val Loss: 1.6925 | Val Acc: 43.75%
Precision: 0.3829 | Recall: 0.4196 | F1 Score: 0.3766
Current AMP scale: 16384.0
Unique augmented volumes seen in epoch 31: 55
Label distribution in training epoch: Counter({0: 51, 1: 42})

Validation loss did not improve. Patience: 27/50

Epoch 32/100
Train Loss: 0.1041 | Train Acc: 97.85%
Val Loss: 1.2253 | Val Acc: 56.25%
Precision: 0.5317 | Recall: 0.5417 | F1 Score: 0.5152
Current AMP scale: 16384.0
Unique augmented volumes seen in epoch 32: 60
Label distribution in training epoch: Counter({1: 50, 0: 43})

Validation loss did not improve. Patience: 28/50

Epoch 33/100
Train Loss: 0.1098 | Train Acc: 95.70%
Val Loss: 1.7648 | Val Acc: 56.25%
Precision: 0.5587 | Recall: 0.5569 | F1 Score: 0.5556
Current AMP scale: 16384.0
Unique augmented volumes seen in epoch 33: 62
Label distribution in training epoch: Counter({1: 52, 0: 41})

Validation loss did not improve. Patience: 29/50

Epoch 34/100
Train Loss: 0.1866 | Train Acc: 91.40%
Val Loss: 0.9518 | Val Acc: 68.75%
Precision: 0.6984 | Recall: 0.6984 | F1 Score: 0.6875
Current AMP scale: 16384.0
Unique augmented volumes seen in epoch 34: 58
Label distribution in training epoch: Counter({1: 47, 0: 46})

Validation loss did not improve. Patience: 30/50

Epoch 35/100
Train Loss: 0.2104 | Train Acc: 92.47%
Val Loss: 1.7100 | Val Acc: 34.38%
Precision: 0.3250 | Recall: 0.3353 | F1 Score: 0.3273
Current AMP scale: 16384.0
Unique augmented volumes seen in epoch 35: 58
Label distribution in training epoch: Counter({1: 49, 0: 44})

Validation loss did not improve. Patience: 31/50

Epoch 36/100
Train Loss: 0.2677 | Train Acc: 92.47%
Val Loss: 2.1711 | Val Acc: 46.88%
Precision: 0.4792 | Recall: 0.4843 | F1 Score: 0.4421
Current AMP scale: 16384.0
Unique augmented volumes seen in epoch 36: 61
Label distribution in training epoch: Counter({1: 48, 0: 45})

Validation loss did not improve. Patience: 32/50

Epoch 37/100
Train Loss: 0.1867 | Train Acc: 92.47%
Val Loss: 1.9896 | Val Acc: 31.25%
Precision: 0.3098 | Recall: 0.3098 | F1 Score: 0.3098
Current AMP scale: 16384.0
Unique augmented volumes seen in epoch 37: 60
Label distribution in training epoch: Counter({1: 51, 0: 42})

Validation loss did not improve. Patience: 33/50

Epoch 38/100
Train Loss: 0.2861 | Train Acc: 86.02%
Val Loss: 1.8476 | Val Acc: 56.25%
Precision: 0.6280 | Recall: 0.6073 | F1 Score: 0.5556
Current AMP scale: 16384.0
Unique augmented volumes seen in epoch 38: 58
Label distribution in training epoch: Counter({0: 57, 1: 36})

Validation loss did not improve. Patience: 34/50

Epoch 39/100
Train Loss: 0.0448 | Train Acc: 100.00%
Val Loss: 1.9966 | Val Acc: 46.88%
Precision: 0.4654 | Recall: 0.4688 | F1 Score: 0.4555
Current AMP scale: 16384.0
Unique augmented volumes seen in epoch 39: 60
Label distribution in training epoch: Counter({1: 52, 0: 41})

Validation loss did not improve. Patience: 35/50

Epoch 40/100
Train Loss: 0.1902 | Train Acc: 93.55%
Val Loss: 2.4143 | Val Acc: 43.75%
Precision: 0.4643 | Recall: 0.4841 | F1 Score: 0.3766
Current AMP scale: 16384.0
Unique augmented volumes seen in epoch 40: 52
Label distribution in training epoch: Counter({1: 54, 0: 39})

Validation loss did not improve. Patience: 36/50

Epoch 41/100
Train Loss: 0.2384 | Train Acc: 93.55%
Val Loss: 3.9186 | Val Acc: 34.38%
Precision: 0.3397 | Recall: 0.3988 | F1 Score: 0.3108
Current AMP scale: 16384.0
Unique augmented volumes seen in epoch 41: 53
Label distribution in training epoch: Counter({0: 54, 1: 39})

Validation loss did not improve. Patience: 37/50

Epoch 42/100
Train Loss: 0.1780 | Train Acc: 90.32%
Val Loss: 1.2140 | Val Acc: 46.88%
Precision: 0.4437 | Recall: 0.4484 | F1 Score: 0.4421
Current AMP scale: 16384.0
Unique augmented volumes seen in epoch 42: 58
Label distribution in training epoch: Counter({0: 49, 1: 44})

Validation loss did not improve. Patience: 38/50

Epoch 43/100
Train Loss: 0.1126 | Train Acc: 97.85%
Val Loss: 1.1883 | Val Acc: 53.12%
Precision: 0.5779 | Recall: 0.5750 | F1 Score: 0.5308
Current AMP scale: 16384.0
Unique augmented volumes seen in epoch 43: 61
Label distribution in training epoch: Counter({0: 50, 1: 43})

Validation loss did not improve. Patience: 39/50

Epoch 44/100
Train Loss: 0.2005 | Train Acc: 94.62%
Val Loss: 1.8546 | Val Acc: 40.62%
Precision: 0.4271 | Recall: 0.4250 | F1 Score: 0.4057
Current AMP scale: 16384.0
Unique augmented volumes seen in epoch 44: 52
Label distribution in training epoch: Counter({1: 52, 0: 41})

Validation loss did not improve. Patience: 40/50

Epoch 45/100
Train Loss: 0.1918 | Train Acc: 93.55%
Val Loss: 1.9523 | Val Acc: 53.12%
Precision: 0.5725 | Recall: 0.5595 | F1 Score: 0.5195
Current AMP scale: 16384.0
Unique augmented volumes seen in epoch 45: 58
Label distribution in training epoch: Counter({0: 52, 1: 41})

Validation loss did not improve. Patience: 41/50

Epoch 46/100
Train Loss: 0.1017 | Train Acc: 94.62%
Val Loss: 1.9375 | Val Acc: 46.88%
Precision: 0.4221 | Recall: 0.4250 | F1 Score: 0.4231
Current AMP scale: 16384.0
Unique augmented volumes seen in epoch 46: 56
Label distribution in training epoch: Counter({0: 49, 1: 44})

Validation loss did not improve. Patience: 42/50

Epoch 47/100
Train Loss: 0.0768 | Train Acc: 96.77%
Val Loss: 1.2102 | Val Acc: 56.25%
Precision: 0.5714 | Recall: 0.5714 | F1 Score: 0.5625
Current AMP scale: 16384.0
Unique augmented volumes seen in epoch 47: 55
Label distribution in training epoch: Counter({0: 51, 1: 42})

Validation loss did not improve. Patience: 43/50

Epoch 48/100
Train Loss: 0.1521 | Train Acc: 94.62%
Val Loss: 1.6654 | Val Acc: 43.75%
Precision: 0.4392 | Recall: 0.4392 | F1 Score: 0.4375
Current AMP scale: 16384.0
Unique augmented volumes seen in epoch 48: 61
Label distribution in training epoch: Counter({0: 48, 1: 45})

Validation loss did not improve. Patience: 44/50

Epoch 49/100
Train Loss: 0.0635 | Train Acc: 97.85%
Val Loss: 1.1675 | Val Acc: 59.38%
Precision: 0.5409 | Recall: 0.5390 | F1 Score: 0.5393
Current AMP scale: 16384.0
Unique augmented volumes seen in epoch 49: 61
Label distribution in training epoch: Counter({1: 47, 0: 46})

Validation loss did not improve. Patience: 45/50

Epoch 50/100
Train Loss: 0.1791 | Train Acc: 94.62%
Val Loss: 1.0818 | Val Acc: 53.12%
Precision: 0.5500 | Recall: 0.5431 | F1 Score: 0.5195
Current AMP scale: 16384.0
Unique augmented volumes seen in epoch 50: 64
Label distribution in training epoch: Counter({1: 47, 0: 46})

Validation loss did not improve. Patience: 46/50

Epoch 51/100
Train Loss: 0.0418 | Train Acc: 98.92%
Val Loss: 1.4443 | Val Acc: 56.25%
Precision: 0.6280 | Recall: 0.6073 | F1 Score: 0.5556
Current AMP scale: 16384.0
Unique augmented volumes seen in epoch 51: 60
Label distribution in training epoch: Counter({0: 51, 1: 42})

Validation loss did not improve. Patience: 47/50

Epoch 52/100
Train Loss: 0.0234 | Train Acc: 100.00%
Val Loss: 1.3262 | Val Acc: 50.00%
Precision: 0.5771 | Recall: 0.5547 | F1 Score: 0.4818
Current AMP scale: 16384.0
Unique augmented volumes seen in epoch 52: 62
Label distribution in training epoch: Counter({0: 50, 1: 43})

Validation loss did not improve. Patience: 48/50

Epoch 53/100
Train Loss: 0.1570 | Train Acc: 95.70%
Val Loss: 2.2454 | Val Acc: 43.75%
Precision: 0.2500 | Recall: 0.3889 | F1 Score: 0.3043
Current AMP scale: 16384.0
Unique augmented volumes seen in epoch 53: 67
Label distribution in training epoch: Counter({1: 52, 0: 41})

Validation loss did not improve. Patience: 49/50

Epoch 54/100
Train Loss: 0.1958 | Train Acc: 92.47%
Val Loss: 1.5550 | Val Acc: 56.25%
Precision: 0.5385 | Recall: 0.5238 | F1 Score: 0.4909
Current AMP scale: 16384.0
Unique augmented volumes seen in epoch 54: 57
Label distribution in training epoch: Counter({0: 48, 1: 45})

Validation loss did not improve. Patience: 50/50

Early stopping triggered after 54 epochs.


Training complete.
Loading best model from /home/etudiant/Projets/Viviane/LIDC-ML/models/best_model_resnet_pytorch3D_architecture_v0.pth for final metrics.
######## Training Finished in 0h 22m 49s ###########
Test Accuracy on 32 images: 50.00%
AUC: 0.5833
Class 0-non-cancer: Precision: 0.50, Recall: 0.42, F1-Score: 0.46
Class 1-cancer: Precision: 0.31, Recall: 0.38, F1-Score: 0.34
