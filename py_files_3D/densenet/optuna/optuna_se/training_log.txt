

==== Sampling started at 2025-06-25 00:52:53.133908 ====


Starting Optuna optimization for DenseNet3DWithSE...

Optimization finished!
Number of finished trials: 50
Number of pruned trials: 35
Number of complete trials: 15

Best trial:FrozenTrial(number=10, state=1, values=[0.6875], datetime_start=datetime.datetime(2025, 6, 25, 1, 2, 33, 280842), datetime_complete=datetime.datetime(2025, 6, 25, 1, 5, 43, 50290), params={'lr': 0.0005817376301045428, 'optimizer': 'Adam', 'batch_size': 4, 'epochs': 20, 'weight_decay': 1.2956565410409286e-06, 'gradient_clipping_norm': 5.0, 'se_reduction': 24, 'dropout_rate': 0.4}, user_attrs={}, system_attrs={}, intermediate_values={0: 0.65625, 1: 0.5, 2: 0.4375, 3: 0.5625, 4: 0.5, 5: 0.46875, 6: 0.53125, 7: 0.53125, 8: 0.375, 9: 0.46875, 10: 0.5625, 11: 0.40625, 12: 0.375, 13: 0.5625, 14: 0.34375, 15: 0.40625, 16: 0.59375, 17: 0.40625, 18: 0.375, 19: 0.6875}, distributions={'lr': FloatDistribution(high=0.01, log=True, low=1e-05, step=None), 'optimizer': CategoricalDistribution(choices=('Adam', 'RMSprop', 'SGD')), 'batch_size': CategoricalDistribution(choices=(1, 2, 4, 8)), 'epochs': IntDistribution(high=20, log=False, low=5, step=1), 'weight_decay': FloatDistribution(high=0.001, log=True, low=1e-06, step=None), 'gradient_clipping_norm': FloatDistribution(high=5.0, log=False, low=0.1, step=0.1), 'se_reduction': IntDistribution(high=32, log=False, low=8, step=4), 'dropout_rate': FloatDistribution(high=0.5, log=False, low=0.1, step=0.1)}, trial_id=10, value=None)
    lr: 0.0005817376301045428
    optimizer: Adam
    batch_size: 4
    epochs: 20
    weight_decay: 1.2956565410409286e-06
    gradient_clipping_norm: 5.0
    se_reduction: 24
    dropout_rate: 0.4
######## Training LR Finished in 0h 38m 43s ###########
